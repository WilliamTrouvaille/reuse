# ============================================================
# 优化器配置 (Optimizers)
# ============================================================
# 本配置文件用于 utils/optimizers 模块和 PyTorch 内置优化器
# 提供多种优化器及其超参数配置
optimizer:
    # 优化器类型选择
    # 可选值: 'SGD', 'Adam', 'AdamW', 'AdamP', 'Lion', 'MadGrad'
    # 依赖:
    #   - PyTorch 内置: torch.optim (SGD, Adam, AdamW)
    #   - 自定义实现: utils.optimizers (AdamP, Lion, MadGrad)
    #
    # 推荐选择指南:
    #   - SGD: 经典优化器，收敛稳定但需精细调参
    #          适用场景: ResNet 等经典模型、充足调参时间
    #          论文: Bottou (1991)
    #   - Adam: 自适应学习率，收敛快但泛化可能较差
    #           适用场景: Transformer、快速实验
    #           论文: Kingma & Ba (2015)
    #   - AdamW: Adam + 解耦权重衰减，泛化更好（推荐）
    #            适用场景: 大多数场景的首选
    #            论文: Loshchilov & Hutter (2019)
    #   - AdamP: Adam + 投影约束，抑制权重范数膨胀
    #            适用场景: 深层网络、长时间训练
    #            论文: Heo et al. (2020)
    #   - Lion: 符号动量优化器，内存效率高
    #           适用场景: 大模型、显存受限
    #           论文: Chen et al. (2023)
    #   - MadGrad: 动量化自适应双平均，鲁棒性强
    #              适用场景: 学习率敏感的任务
    #              论文: Defazio & Jelassi (2021)
    type: "AdamW"

    # ========================================================
    # 通用参数（适用于所有优化器）
    # ========================================================

    # 初始学习率
    # 推荐范围:
    #   - SGD: 0.01 - 0.1
    #   - Adam/AdamW: 0.0001 - 0.001
    #   - AdamP: 0.0001 - 0.001
    #   - Lion: 0.00001 - 0.0001 (通常是 Adam 的 1/3 到 1/10)
    #   - MadGrad: 0.001 - 0.01
    # 注意: 不同优化器对学习率的敏感度不同
    lr: 0.001

    # L2 正则化系数（权重衰减）
    # 推荐范围:
    #   - 一般任务: 0.0001 - 0.001
    #   - 小数据集: 0.001 - 0.01 (更强正则化)
    #   - 大数据集: 0.00001 - 0.0001 (较弱正则化)
    # 注意: AdamW 使用解耦权重衰减，效果更稳定
    weight_decay: 0.0001

    # ========================================================
    # SGD 专用参数
    # ========================================================
    # 依赖: torch.optim.SGD
    sgd:
        # 动量系数
        # 推荐: 0.9（标准设置）
        # 取值范围: [0.0, 1.0]
        # 作用: 加速收敛、减少震荡
        momentum: 0.9

        # 是否使用 Nesterov 动量
        # 推荐: true（收敛更快）
        # 论文: Nesterov (1983)
        nesterov: true

        # 动量阻尼
        # 推荐: 0.0（标准设置）
        # 取值范围: [0.0, 1.0]
        # 作用: 抑制动量过大
        dampening: 0.0

    # ========================================================
    # Adam / AdamW 专用参数
    # ========================================================
    # 依赖: torch.optim.Adam, torch.optim.AdamW
    adam:
        # 一阶和二阶矩估计的指数衰减率
        # 推荐: [0.9, 0.999]（标准设置）
        # betas[0]: 一阶矩（梯度）的衰减率
        #   - 较小值（如 0.8）: 对最近梯度更敏感
        #   - 较大值（如 0.95）: 更平滑的梯度估计
        # betas[1]: 二阶矩（梯度平方）的衰减率
        #   - 通常保持 0.999 不变
        betas: [0.9, 0.999]

        # 数值稳定性常数（防止除零）
        # 推荐: 1e-8（标准设置）
        # 取值范围: [1e-10, 1e-6]
        # 注意: 过小可能导致数值不稳定
        eps: 1.0e-8

        # 是否使用 AMSGrad 变体
        # 推荐: false（标准 Adam 已足够）
        # 适用场景: 训练不稳定、需要更强的二阶矩估计
        # 论文: Reddi et al. (2018)
        amsgrad: false

    # ========================================================
    # AdamP 专用参数
    # ========================================================
    # 依赖: utils.optimizers.AdamP
    # 论文: Slowing Down the Weight Norm Increase in Momentum-based Optimizers (CVPR 2021)
    adamp:
        # 一阶和二阶矩估计的指数衰减率
        # 推荐: [0.9, 0.999]（与 Adam 相同）
        betas: [0.9, 0.999]

        # 数值稳定性常数
        # 推荐: 1e-8
        eps: 1.0e-8

        # 投影半径（关键参数）
        # 推荐: 0.1（标准设置）
        # 取值范围: [0.01, 1.0]
        # 作用: 控制权重范数增长速度
        #   - 较小值（如 0.01）: 强约束，适用于深层网络
        #   - 较大值（如 1.0）: 弱约束，接近标准 Adam
        delta: 0.1

        # 权重衰减比率（关键参数）
        # 推荐: 0.1（标准设置）
        # 取值范围: [0.0, 1.0]
        # 作用: 控制权重衰减在投影中的权重
        #   - 0.0: 完全忽略权重衰减
        #   - 1.0: 完全使用权重衰减
        wd_ratio: 0.1

        # 是否使用 Nesterov 动量
        # 推荐: true（收敛更快）
        nesterov: true

    # ========================================================
    # Lion 专用参数
    # ========================================================
    # 依赖: utils.optimizers.Lion
    # 论文: Symbolic Discovery of Optimization Algorithms (ICML 2023)
    # 特点: 使用符号运算代替指数移动平均，内存效率高
    lion:
        # 动量参数
        # 推荐: [0.9, 0.99]（标准设置）
        # 注意: 与 Adam 的 betas 语义不同
        # betas[0]: 更新时使用的动量
        # betas[1]: 计算梯度时使用的动量
        betas: [0.9, 0.99]

        # 重要: Lion 的学习率通常比 Adam 小 1/3 到 1/10
        # 推荐学习率: 0.0001 (如果 Adam 使用 0.001)
        # 原因: Lion 使用符号梯度，步长更激进

        # Lion 特点:
        #   - 优点: 内存占用低（约为 Adam 的 50%）、收敛快
        #   - 缺点: 对学习率敏感、可能需要额外的权重衰减

    # ========================================================
    # MadGrad 专用参数
    # ========================================================
    # 依赖: utils.optimizers.MadGrad
    # 论文: Adaptivity without Compromise (NeurIPS 2021)
    # 特点: 动量化自适应双平均，对学习率不敏感
    madgrad:
        # 动量系数
        # 推荐: 0.9（标准设置）
        # 取值范围: [0.0, 1.0]
        # 作用: 平滑梯度估计
        momentum: 0.9

        # 数值稳定性常数
        # 推荐: 1e-6（比 Adam 略大）
        # 取值范围: [1e-8, 1e-5]
        eps: 1.0e-6

        # MadGrad 特点:
        #   - 优点: 对学习率不敏感、鲁棒性强
        #   - 缺点: 收敛速度略慢于 AdamW
        #   - 适用场景: 学习率难以调优的任务
