# `data` æ•°æ®åŠ è½½æ¨¡å—ä½¿ç”¨æŒ‡å—

## ğŸ“š ç›®å½•
1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ”¯æŒçš„æ•°æ®é›†](#æ”¯æŒçš„æ•°æ®é›†)
3. [é…ç½®é€‰é¡¹](#é…ç½®é€‰é¡¹)
4. [å¸¸è§åœºæ™¯](#å¸¸è§åœºæ™¯)
5. [è¿”å›å€¼è¯´æ˜](#è¿”å›å€¼è¯´æ˜)
6. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€å°åŒ–ç¤ºä¾‹ï¼ˆ2 è¡Œä»£ç ï¼‰

```python
from utils import load_dataset_info

# åŠ è½½ CIFAR10 æ•°æ®é›†
dataset_info = load_dataset_info(dataset_name='CIFAR10', data_path='./data')

# ç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨
train_dataset = dataset_info['dst_train']
test_dataset = dataset_info['dst_test']
num_classes = dataset_info['num_classes']  # 10
```

å°±è¿™ä¹ˆç®€å•ï¼`load_dataset_info` ä¼šè‡ªåŠ¨ï¼š
- âœ… ä¸‹è½½æ•°æ®é›†ï¼ˆå¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼‰
- âœ… åº”ç”¨æ ‡å‡†å½’ä¸€åŒ–å˜æ¢
- âœ… è¿”å›è®­ç»ƒé›†å’Œæµ‹è¯•é›†
- âœ… æä¾›å®Œæ•´çš„æ•°æ®é›†å…ƒæ•°æ®

---

## ğŸ“¦ æ”¯æŒçš„æ•°æ®é›†

### å½“å‰æ”¯æŒçš„æ•°æ®é›†

| æ•°æ®é›† | å›¾åƒå°ºå¯¸ | é€šé“ | ç±»åˆ«æ•° | ç”¨é€” |
|--------|---------|------|--------|------|
| `MNIST` | 28Ã—28 | 1 (ç°åº¦) | 10 | æ‰‹å†™æ•°å­—è¯†åˆ« |
| `FashionMNIST` | 28Ã—28 | 1 (ç°åº¦) | 10 | æ—¶å°šç‰©å“åˆ†ç±» |
| `CIFAR10` | 32Ã—32 | 3 (RGB) | 10 | é€šç”¨ç‰©ä½“è¯†åˆ« |
| `CIFAR100` | 32Ã—32 | 3 (RGB) | 100 | ç»†ç²’åº¦ç‰©ä½“è¯†åˆ« |

### æ•°æ®é›†ç‰¹ç‚¹

**MNIST**ï¼š
- ç»å…¸çš„æ‰‹å†™æ•°å­—æ•°æ®é›†ï¼ˆ0-9ï¼‰
- è®­ç»ƒé›†ï¼š60,000 å¼ å›¾åƒ
- æµ‹è¯•é›†ï¼š10,000 å¼ å›¾åƒ
- é€‚åˆå¿«é€ŸåŸå‹éªŒè¯

**FashionMNIST**ï¼š
- MNIST çš„æ—¶å°šç‰©å“ç‰ˆæœ¬
- ç±»åˆ«ï¼šTæ¤ã€è£¤å­ã€å¥—è¡«ã€è£™å­ç­‰
- æ¯” MNIST æ›´å…·æŒ‘æˆ˜æ€§
- è®­ç»ƒé›†ï¼š60,000 å¼ å›¾åƒ
- æµ‹è¯•é›†ï¼š10,000 å¼ å›¾åƒ

**CIFAR10**ï¼š
- å½©è‰²å›¾åƒï¼Œ10 ä¸ªç±»åˆ«
- ç±»åˆ«ï¼šé£æœºã€æ±½è½¦ã€é¸Ÿã€çŒ«ã€é¹¿ã€ç‹—ã€é’è›™ã€é©¬ã€èˆ¹ã€å¡è½¦
- è®­ç»ƒé›†ï¼š50,000 å¼ å›¾åƒ
- æµ‹è¯•é›†ï¼š10,000 å¼ å›¾åƒ
- é€‚åˆä¸­ç­‰è§„æ¨¡å®éªŒ

**CIFAR100**ï¼š
- CIFAR10 çš„ç»†ç²’åº¦ç‰ˆæœ¬
- 100 ä¸ªç±»åˆ«ï¼ˆ20 ä¸ªå¤§ç±»ï¼Œæ¯ä¸ªå¤§ç±» 5 ä¸ªå°ç±»ï¼‰
- è®­ç»ƒé›†ï¼š50,000 å¼ å›¾åƒ
- æµ‹è¯•é›†ï¼š10,000 å¼ å›¾åƒ
- é€‚åˆå¤šåˆ†ç±»ä»»åŠ¡

---

## âš™ï¸ é…ç½®é€‰é¡¹

### åŸºç¡€ç”¨æ³•

```python
from utils import load_dataset_info

dataset_info = load_dataset_info(
    dataset_name='CIFAR10',  # æ•°æ®é›†åç§°
    data_path='./data'       # æ•°æ®å­˜å‚¨è·¯å¾„
)
```

**å‚æ•°è¯´æ˜**ï¼š
- `dataset_name` (str): æ•°æ®é›†åç§°ï¼Œå¿…é¡»æ˜¯æ”¯æŒçš„æ•°æ®é›†ä¹‹ä¸€
- `data_path` (str): æ•°æ®é›†ä¸‹è½½å’Œå­˜å‚¨çš„æ ¹è·¯å¾„

### åœ¨é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨

æ¨èåœ¨ `config.yaml` ä¸­é…ç½®æ•°æ®é›†å‚æ•°ï¼š

```yaml
# config.yaml
dataset:
  name: "CIFAR10"
  data_path: "./data"

dataloader:
  num_workers: 4
  pin_memory: true
  batch_size: 128
```

ç„¶ååœ¨ä»£ç ä¸­ä½¿ç”¨ï¼š

```python
from utils import setup_config, load_dataset_info

# åŠ è½½é…ç½®
config = setup_config(DEFAULT_CONFIG, 'config.yaml', {})

# ä»é…ç½®åŠ è½½æ•°æ®é›†
dataset_info = load_dataset_info(
    dataset_name=config.dataset.name,
    data_path=config.dataset.data_path
)
```

---

## ğŸ“– å¸¸è§åœºæ™¯

### åœºæ™¯ 1: åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨

```python
from torch.utils.data import DataLoader
from utils import load_dataset_info, setup_config
from loguru import logger

def main():
    # 1. åŠ è½½é…ç½®
    config = setup_config(DEFAULT_CONFIG, 'config.yaml', {})

    # 2. åŠ è½½æ•°æ®é›†
    dataset_info = load_dataset_info(
        dataset_name=config.dataset.name,
        data_path=config.dataset.data_path
    )

    # 3. åˆ›å»º DataLoader
    train_loader = DataLoader(
        dataset_info['dst_train'],
        batch_size=config.training.batch_size,
        shuffle=True,
        num_workers=config.dataloader.num_workers,
        pin_memory=config.dataloader.pin_memory
    )

    val_loader = DataLoader(
        dataset_info['dst_test'],
        batch_size=config.training.batch_size,
        shuffle=False,
        num_workers=config.dataloader.num_workers,
        pin_memory=config.dataloader.pin_memory
    )

    # 4. ä½¿ç”¨æ•°æ®é›†å…ƒæ•°æ®
    logger.info(f"æ•°æ®é›†: {config.dataset.name}")
    logger.info(f"ç±»åˆ«æ•°: {dataset_info['num_classes']}")
    logger.info(f"å›¾åƒå°ºå¯¸: {dataset_info['im_size']}")
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(dataset_info['dst_train'])}")
    logger.info(f"æµ‹è¯•é›†å¤§å°: {len(dataset_info['dst_test'])}")

    # 5. å¼€å§‹è®­ç»ƒ...
    trainer.fit(train_loader, val_loader, epochs=100)
```

### åœºæ™¯ 2: å¿«é€Ÿå®éªŒï¼ˆä¸ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰

```python
from torch.utils.data import DataLoader
from utils import load_dataset_info

# ç›´æ¥åŠ è½½æ•°æ®é›†
dataset_info = load_dataset_info('MNIST', './data')

# å¿«é€Ÿåˆ›å»º DataLoader
train_loader = DataLoader(
    dataset_info['dst_train'],
    batch_size=64,
    shuffle=True
)

test_loader = DataLoader(
    dataset_info['dst_test'],
    batch_size=64,
    shuffle=False
)

# å¼€å§‹è®­ç»ƒ...
for images, labels in train_loader:
    # ... è®­ç»ƒä»£ç  ...
    pass
```

### åœºæ™¯ 3: æ•°æ®æ¢ç´¢

```python
from utils import load_dataset_info
import matplotlib.pyplot as plt

# åŠ è½½æ•°æ®é›†
dataset_info = load_dataset_info('CIFAR10', './data')

# æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯
print(f"ç±»åˆ«åç§°: {dataset_info['class_names']}")
print(f"å‡å€¼: {dataset_info['mean']}")
print(f"æ ‡å‡†å·®: {dataset_info['std']}")

# å¯è§†åŒ–æ ·æœ¬
train_dataset = dataset_info['dst_train']
image, label = train_dataset[0]

plt.imshow(image.permute(1, 2, 0))  # è½¬æ¢ä¸º (H, W, C)
plt.title(f"Label: {dataset_info['class_names'][label]}")
plt.show()
```

### åœºæ™¯ 4: åˆ‡æ¢æ•°æ®é›†ï¼ˆå¿«é€Ÿå¯¹æ¯”å®éªŒï¼‰

```python
from utils import load_dataset_info

# åªéœ€ä¿®æ”¹ä¸€ä¸ªå‚æ•°å³å¯åˆ‡æ¢æ•°æ®é›†
datasets_to_test = ['MNIST', 'FashionMNIST', 'CIFAR10']

for dataset_name in datasets_to_test:
    print(f"\næµ‹è¯•æ•°æ®é›†: {dataset_name}")

    dataset_info = load_dataset_info(dataset_name, './data')

    # ç»Ÿä¸€çš„è®­ç»ƒæµç¨‹
    train_loader = create_dataloader(dataset_info['dst_train'])
    test_loader = create_dataloader(dataset_info['dst_test'])

    # ä½¿ç”¨æ•°æ®é›†å…ƒæ•°æ®é…ç½®æ¨¡å‹
    model = create_model(
        input_channels=dataset_info['channel'],
        num_classes=dataset_info['num_classes']
    )

    # è®­ç»ƒ...
    train(model, train_loader, test_loader)
```

---

## ğŸ“Š è¿”å›å€¼è¯´æ˜

### å®Œæ•´è¿”å›å­—å…¸

`load_dataset_info()` è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®å€¼ï¼š

```python
{
    'dst_train': Dataset,      # PyTorch Dataset å¯¹è±¡ï¼ˆè®­ç»ƒé›†ï¼‰
    'dst_test': Dataset,       # PyTorch Dataset å¯¹è±¡ï¼ˆæµ‹è¯•é›†ï¼‰
    'im_size': tuple,          # å›¾åƒå°ºå¯¸ (H, W)ï¼Œä¾‹å¦‚ (32, 32)
    'channel': int,            # é€šé“æ•°ï¼Œ1 (ç°åº¦) æˆ– 3 (RGB)
    'num_classes': int,        # ç±»åˆ«æ•°
    'class_names': list,       # ç±»åˆ«åç§°åˆ—è¡¨
    'mean': list,              # å½’ä¸€åŒ–å‡å€¼ï¼ˆæ¯ä¸ªé€šé“ï¼‰
    'std': list                # å½’ä¸€åŒ–æ ‡å‡†å·®ï¼ˆæ¯ä¸ªé€šé“ï¼‰
}
```

### å­—æ®µè¯¦è§£

**æ ¸å¿ƒæ•°æ®é›†å¯¹è±¡**ï¼š
- `dst_train`: è®­ç»ƒé›†ï¼Œå¯ç›´æ¥ä¼ ç»™ `DataLoader`
- `dst_test`: æµ‹è¯•é›†ï¼ˆæˆ–éªŒè¯é›†ï¼‰ï¼Œå¯ç›´æ¥ä¼ ç»™ `DataLoader`

**å…ƒæ•°æ®å­—æ®µ**ï¼š
- `im_size`: å›¾åƒå°ºå¯¸ï¼ˆé«˜åº¦, å®½åº¦ï¼‰ï¼Œç”¨äºæ¨¡å‹è¾“å…¥é…ç½®
- `channel`: é€šé“æ•°ï¼Œç”¨äºæ¨¡å‹ç¬¬ä¸€å±‚å·ç§¯é…ç½®
- `num_classes`: ç±»åˆ«æ•°ï¼Œç”¨äºæ¨¡å‹æœ€åä¸€å±‚å…¨è¿æ¥é…ç½®
- `class_names`: ç±»åˆ«åç§°ï¼Œç”¨äºå¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ

**å½’ä¸€åŒ–å‚æ•°**ï¼ˆå·²è‡ªåŠ¨åº”ç”¨ï¼‰ï¼š
- `mean`: æ¯ä¸ªé€šé“çš„å‡å€¼
- `std`: æ¯ä¸ªé€šé“çš„æ ‡å‡†å·®

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. æ‰©å±•æ”¯æŒçš„æ•°æ®é›†

å¦‚æœéœ€è¦æ·»åŠ æ–°çš„æ•°æ®é›†ï¼Œåªéœ€åœ¨ `utils/data.py` ä¸­ä¿®æ”¹æ³¨å†Œè¡¨ï¼š

```python
# utils/data.py

_DATASET_REGISTRY = {
    # ... ç°æœ‰æ•°æ®é›† ...

    'YourDataset': {
        'torchvision_class': datasets.YourDataset,  # torchvision ç±»
        'im_size': (224, 224),                      # å›¾åƒå°ºå¯¸
        'channel': 3,                               # é€šé“æ•°
        'num_classes': 1000,                        # ç±»åˆ«æ•°
        'mean': [0.485, 0.456, 0.406],             # ImageNet å‡å€¼
        'std': [0.229, 0.224, 0.225],              # ImageNet æ ‡å‡†å·®
    }
}
```

ç„¶åå°±å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š

```python
dataset_info = load_dataset_info('YourDataset', './data')
```

### 2. è‡ªå®šä¹‰æ•°æ®å¢å¼º

å¦‚æœéœ€è¦è‡ªå®šä¹‰æ•°æ®å¢å¼ºï¼Œå¯ä»¥åœ¨è·å–æ•°æ®é›†åé‡æ–°è®¾ç½® `transform`ï¼š

```python
from torchvision import transforms
from utils import load_dataset_info

# å…ˆåŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨é»˜è®¤å½’ä¸€åŒ–ï¼‰
dataset_info = load_dataset_info('CIFAR10', './data')

# è‡ªå®šä¹‰è®­ç»ƒé›†å¢å¼º
train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),      # éšæœºè£å‰ª
    transforms.RandomHorizontalFlip(),         # éšæœºæ°´å¹³ç¿»è½¬
    transforms.ToTensor(),
    transforms.Normalize(
        mean=dataset_info['mean'],
        std=dataset_info['std']
    )
])

# é‡æ–°è®¾ç½®å˜æ¢
dataset_info['dst_train'].transform = train_transform

# æµ‹è¯•é›†ä¿æŒé»˜è®¤ï¼ˆå·²ç»æ˜¯å½’ä¸€åŒ–ï¼‰
# dataset_info['dst_test'] æ— éœ€ä¿®æ”¹
```

### 3. ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

å¦‚æœä½¿ç”¨å®Œå…¨è‡ªå®šä¹‰çš„æ•°æ®é›†ï¼ˆä¸åœ¨ torchvision ä¸­ï¼‰ï¼Œå¯ä»¥ï¼š

**æ–¹å¼ 1**ï¼šç›´æ¥ä½¿ç”¨ï¼Œä¸è°ƒç”¨ `load_dataset_info`

```python
from torch.utils.data import Dataset, DataLoader

class MyCustomDataset(Dataset):
    def __init__(self, ...):
        # è‡ªå®šä¹‰åˆå§‹åŒ–
        pass

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# ç›´æ¥åˆ›å»º DataLoader
train_dataset = MyCustomDataset(...)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

**æ–¹å¼ 2**ï¼šæ‰©å±• `_DATASET_REGISTRY`ï¼ˆæ¨èï¼Œç»Ÿä¸€æ¥å£ï¼‰

```python
# utils/data.py

from your_module import CustomDataset

_DATASET_REGISTRY['CustomDataset'] = {
    'torchvision_class': CustomDataset,  # ä½ çš„è‡ªå®šä¹‰ç±»
    'im_size': (256, 256),
    'channel': 3,
    'num_classes': 50,
    'mean': [0.5, 0.5, 0.5],
    'std': [0.5, 0.5, 0.5],
}
```

### 4. è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯

```python
from utils import load_dataset_info
from loguru import logger

dataset_info = load_dataset_info('CIFAR10', './data')

# æ‰“å°å®Œæ•´çš„æ•°æ®é›†ä¿¡æ¯
logger.info("=" * 60)
logger.info("æ•°æ®é›†ä¿¡æ¯".center(60))
logger.info("=" * 60)
logger.info(f"åç§°: CIFAR10")
logger.info(f"è®­ç»ƒé›†å¤§å°: {len(dataset_info['dst_train'])}")
logger.info(f"æµ‹è¯•é›†å¤§å°: {len(dataset_info['dst_test'])}")
logger.info(f"å›¾åƒå°ºå¯¸: {dataset_info['im_size'][0]}Ã—{dataset_info['im_size'][1]}")
logger.info(f"é€šé“æ•°: {dataset_info['channel']}")
logger.info(f"ç±»åˆ«æ•°: {dataset_info['num_classes']}")
logger.info(f"ç±»åˆ«åç§°: {', '.join(dataset_info['class_names'])}")
logger.info(f"å½’ä¸€åŒ–å‡å€¼: {dataset_info['mean']}")
logger.info(f"å½’ä¸€åŒ–æ ‡å‡†å·®: {dataset_info['std']}")
logger.info("=" * 60)
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: æ•°æ®é›†ä¼šè‡ªåŠ¨ä¸‹è½½å—ï¼Ÿ

**A**: æ˜¯çš„ã€‚å¦‚æœæœ¬åœ° `data_path` ç›®å½•ä¸‹ä¸å­˜åœ¨æ•°æ®é›†ï¼Œ`load_dataset_info` ä¼šè‡ªåŠ¨ä»å®˜æ–¹æºä¸‹è½½ã€‚

```python
# ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä¼šä¸‹è½½ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰
dataset_info = load_dataset_info('CIFAR10', './data')

# ç¬¬äºŒæ¬¡è¿è¡Œæ—¶ä¼šç›´æ¥åŠ è½½æœ¬åœ°æ–‡ä»¶ï¼ˆå¾ˆå¿«ï¼‰
dataset_info = load_dataset_info('CIFAR10', './data')
```

### Q2: ä¸‹è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š

1. **ç½‘ç»œé—®é¢˜**ï¼š
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - å°è¯•ä½¿ç”¨ä»£ç†æˆ– VPN

2. **ç£ç›˜ç©ºé—´ä¸è¶³**ï¼š
   - æ£€æŸ¥ `data_path` æ‰€åœ¨ç£ç›˜æ˜¯å¦æœ‰è¶³å¤Ÿç©ºé—´
   - CIFAR10 çº¦ 170 MBï¼ŒCIFAR100 çº¦ 170 MB

3. **æ‰‹åŠ¨ä¸‹è½½**ï¼š
   - ä»é•œåƒç«™ä¸‹è½½æ•°æ®é›†
   - è§£å‹åˆ° `data_path` ç›®å½•
   - ç¡®ä¿ç›®å½•ç»“æ„æ­£ç¡®

### Q3: å¦‚ä½•æŸ¥çœ‹æ”¯æŒå“ªäº›æ•°æ®é›†ï¼Ÿ

**A**: æœ‰ä¸¤ç§æ–¹æ³•ï¼š

**æ–¹æ³• 1**ï¼šæŸ¥çœ‹é”™è¯¯æç¤º

```python
from utils import load_dataset_info

try:
    dataset_info = load_dataset_info('UnknownDataset', './data')
except ValueError as e:
    print(e)
    # è¾“å‡º: æœªçŸ¥çš„æ•°æ®é›†: UnknownDatasetã€‚æ”¯æŒçš„æ•°æ®é›†: ['MNIST', 'FashionMNIST', 'CIFAR10', 'CIFAR100']
```

**æ–¹æ³• 2**ï¼šæŸ¥çœ‹æºä»£ç 

æ‰“å¼€ `utils/data.py`ï¼ŒæŸ¥çœ‹ `_DATASET_REGISTRY` å­—å…¸çš„é”®ã€‚

### Q4: æ•°æ®æ˜¯å¦‚ä½•å½’ä¸€åŒ–çš„ï¼Ÿ

**A**: ä½¿ç”¨æ¯ä¸ªæ•°æ®é›†é¢„è®¡ç®—çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼š

```python
transform = transforms.Compose([
    transforms.ToTensor(),                    # è½¬æ¢ä¸º Tensorï¼ŒèŒƒå›´ [0, 1]
    transforms.Normalize(mean=..., std=...)  # å½’ä¸€åŒ–ä¸º N(0, 1)
])
```

**å½’ä¸€åŒ–å…¬å¼**ï¼š
```
normalized_value = (original_value - mean) / std
```

**å„æ•°æ®é›†çš„å½’ä¸€åŒ–å‚æ•°**ï¼š
- **MNIST**: mean=[0.1307], std=[0.3081]
- **FashionMNIST**: mean=[0.2861], std=[0.3530]
- **CIFAR10**: mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]
- **CIFAR100**: mean=[0.5071, 0.4866, 0.4409], std=[0.2673, 0.2564, 0.2762]

### Q5: ä¸ºä»€ä¹ˆæ²¡æœ‰éªŒè¯é›†ï¼ˆValidation Setï¼‰ï¼Ÿ

**A**: é»˜è®¤åªè¿”å›è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚å¦‚æœéœ€è¦éªŒè¯é›†ï¼Œå¯ä»¥æ‰‹åŠ¨åˆ’åˆ†ï¼š

```python
from torch.utils.data import random_split
from utils import load_dataset_info

dataset_info = load_dataset_info('CIFAR10', './data')

# å°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºè®­ç»ƒé›† + éªŒè¯é›† (80% / 20%)
train_size = int(0.8 * len(dataset_info['dst_train']))
val_size = len(dataset_info['dst_train']) - train_size

train_dataset, val_dataset = random_split(
    dataset_info['dst_train'],
    [train_size, val_size]
)

# åˆ›å»º DataLoader
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(dataset_info['dst_test'], batch_size=64, shuffle=False)
```

### Q6: å¦‚ä½•è·å–ç±»åˆ«åç§°ï¼Ÿ

**A**: ç›´æ¥ä»è¿”å›å­—å…¸ä¸­è·å–ï¼š

```python
dataset_info = load_dataset_info('CIFAR10', './data')

class_names = dataset_info['class_names']
# ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# ä½¿ç”¨ç±»åˆ«åç§°
for i, name in enumerate(class_names):
    print(f"ç±»åˆ« {i}: {name}")
```

### Q7: DataLoader çš„ num_workers åº”è¯¥è®¾ç½®ä¸ºå¤šå°‘ï¼Ÿ

**A**: æ ¹æ®å¹³å°å’Œ CPU æ ¸å¿ƒæ•°ï¼š

- **Windows**: å»ºè®®è®¾ç½®ä¸º `0`ï¼ˆé¿å…å¤šè¿›ç¨‹é—®é¢˜ï¼‰
- **Linux/Mac**: è®¾ç½®ä¸º `2-8`ï¼ˆæ ¹æ® CPU æ ¸å¿ƒæ•°ï¼‰

```yaml
# config.yaml
dataloader:
  num_workers: 0  # Windows
  # num_workers: 4  # Linux/Mac (4-8 æ ¸ CPU)
```

**æ€§èƒ½æç¤º**ï¼š
- `num_workers > 0` å¯ä»¥å¹¶è¡ŒåŠ è½½æ•°æ®ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
- ä½†åœ¨ Windows ä¸Šå¯èƒ½å¯¼è‡´å¤šè¿›ç¨‹å¯åŠ¨é—®é¢˜
- å»ºè®®åœ¨ Linux æœåŠ¡å™¨ä¸Šè®­ç»ƒæ—¶å¯ç”¨

---

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. æ ‡å‡†è®­ç»ƒæµç¨‹

```python
from utils import setup_config, load_dataset_info, setup_logging
from torch.utils.data import DataLoader
from loguru import logger

def main():
    # 1. é…ç½®æ—¥å¿—
    setup_logging(log_dir='./logs', console_level='INFO', file_level='DEBUG')

    # 2. åŠ è½½é…ç½®
    logger.info("åŠ è½½é…ç½®...")
    config = setup_config(DEFAULT_CONFIG, 'config.yaml', {})

    # 3. åŠ è½½æ•°æ®é›†
    logger.info("åŠ è½½æ•°æ®é›†...")
    dataset_info = load_dataset_info(
        dataset_name=config.dataset.name,
        data_path=config.dataset.data_path
    )

    # 4. åˆ›å»º DataLoader
    train_loader = DataLoader(
        dataset_info['dst_train'],
        batch_size=config.training.batch_size,
        shuffle=True,
        num_workers=config.dataloader.num_workers,
        pin_memory=config.dataloader.pin_memory
    )

    val_loader = DataLoader(
        dataset_info['dst_test'],
        batch_size=config.training.batch_size,
        shuffle=False,
        num_workers=config.dataloader.num_workers,
        pin_memory=config.dataloader.pin_memory
    )

    # 5. åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨æ•°æ®é›†å…ƒæ•°æ®ï¼‰
    logger.info("åˆ›å»ºæ¨¡å‹...")
    model = create_model(
        input_channels=dataset_info['channel'],
        num_classes=dataset_info['num_classes']
    )

    # 6. è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ...")
    trainer.fit(train_loader, val_loader, epochs=config.training.epochs)

    logger.success("è®­ç»ƒå®Œæˆï¼")

if __name__ == '__main__':
    main()
```

### 2. æ•°æ®é›†åˆ‡æ¢æœ€ä½³å®è·µ

å°†æ•°æ®é›†é…ç½®æ”¾åœ¨ YAML æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿å¿«é€Ÿåˆ‡æ¢ï¼š

```yaml
# experiments/mnist.yaml
dataset:
  name: "MNIST"
  data_path: "./data"

# experiments/cifar10.yaml
dataset:
  name: "CIFAR10"
  data_path: "./data"
```

è¿è¡Œæ—¶æŒ‡å®šä¸åŒçš„é…ç½®æ–‡ä»¶ï¼š

```bash
# è®­ç»ƒ MNIST
python main.py --config experiments/mnist.yaml

# è®­ç»ƒ CIFAR10
python main.py --config experiments/cifar10.yaml
```

### 3. è®°å½•æ•°æ®é›†ä¿¡æ¯

```python
from utils import load_dataset_info
from loguru import logger

dataset_info = load_dataset_info('CIFAR10', './data')

# åœ¨è®­ç»ƒå¼€å§‹æ—¶è®°å½•å…³é”®ä¿¡æ¯
logger.info("=" * 60)
logger.info("æ•°æ®é›†é…ç½®".center(60))
logger.info("=" * 60)
logger.info(f"æ•°æ®é›†: CIFAR10")
logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(dataset_info['dst_train'])}")
logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(dataset_info['dst_test'])}")
logger.info(f"ç±»åˆ«æ•°: {dataset_info['num_classes']}")
logger.info("=" * 60)
```

---

## ğŸ¯ æ€»ç»“

`load_dataset_info` çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

1. **ç®€å•æ˜“ç”¨**ï¼šä¸€è¡Œä»£ç åŠ è½½æ•°æ®é›† + å…ƒæ•°æ®
2. **è‡ªåŠ¨ä¸‹è½½**ï¼šæ— éœ€æ‰‹åŠ¨ä¸‹è½½å’Œè§£å‹
3. **æ ‡å‡†å½’ä¸€åŒ–**ï¼šè‡ªåŠ¨åº”ç”¨æœ€ä½³å½’ä¸€åŒ–å‚æ•°
4. **å…ƒæ•°æ®ä¸°å¯Œ**ï¼šæä¾›ç±»åˆ«åç§°ã€å›¾åƒå°ºå¯¸ç­‰
5. **ç»Ÿä¸€æ¥å£**ï¼šæ‰€æœ‰æ•°æ®é›†ä½¿ç”¨ç›¸åŒçš„ API
6. **æ˜“äºæ‰©å±•**ï¼šé€šè¿‡æ³¨å†Œè¡¨æ¨¡å¼æ·»åŠ æ–°æ•°æ®é›†

é…åˆ `DataLoader` å’Œé…ç½®æ–‡ä»¶ï¼Œè®©æ•°æ®åŠ è½½å˜å¾—è½»æ¾æ„‰å¿«ï¼ğŸ‰
