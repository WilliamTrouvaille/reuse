# é…ç½®æ–‡ä»¶ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨å’Œå®šåˆ¶ `config.yaml` é…ç½®æ–‡ä»¶ã€‚

---

## å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»ºé…ç½®æ–‡ä»¶

~~~bash
# å¤åˆ¶æ¨¡æ¿æ–‡ä»¶
cp config.yaml.example config.yaml

# æ ¹æ®éœ€æ±‚ä¿®æ”¹é…ç½®
vim config.yaml  # æˆ–ä½¿ç”¨ä»»ä½•æ–‡æœ¬ç¼–è¾‘å™¨
~~~

### 2. åœ¨ä»£ç ä¸­ä½¿ç”¨

~~~python
from utils import setup_config

# æ–¹å¼1: ä½¿ç”¨é»˜è®¤è·¯å¾„
config = setup_config(
    default_config=get_project_defaults(),
    yaml_config_path='config.yaml',
    cmd_args={}
)

# æ–¹å¼2: é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®š
# python main.py --config my_config.yaml
config = setup_config(
    default_config=get_project_defaults(),
    yaml_config_path=cmd_args['config'],
    cmd_args=cmd_args
)

# è®¿é—®é…ç½®
print(config.training.epochs)  # 100
print(config.dataset.name)     # 'MNIST'
~~~

---

## é…ç½®ä¼˜å…ˆçº§

é…ç½®çš„åŠ è½½éµå¾ªä»¥ä¸‹ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š

```
å‘½ä»¤è¡Œå‚æ•° > config.yaml > ä»£ç é»˜è®¤å€¼
```

### ç¤ºä¾‹

**config.yaml:**
~~~yaml
training:
  epochs: 100
  lr: 0.001
~~~

**å‘½ä»¤è¡Œ:**
~~~bash
python main.py --training.epochs 200 --training.lr 0.01
~~~

**æœ€ç»ˆç»“æœ:**
- `config.training.epochs = 200`  # å‘½ä»¤è¡Œè¦†ç›–
- `config.training.lr = 0.01`     # å‘½ä»¤è¡Œè¦†ç›–
- `config.training.batch_size = 128`  # ä½¿ç”¨ä»£ç é»˜è®¤å€¼

---

## æ ¸å¿ƒé…ç½®é¡¹è¯¦è§£

### 1. å®éªŒé…ç½® (experiment)

~~~yaml
experiment:
  name: "mnist_baseline"  # å®éªŒåç§°ï¼Œç”¨äºæ—¥å¿—æ–‡ä»¶åå’Œé€šçŸ¥
  seed: 42                # éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
  description: "..."      # å®éªŒæè¿°ï¼ˆå¯é€‰ï¼‰
  tags: ["tag1", "tag2"]  # æ ‡ç­¾ï¼Œä¾¿äºç®¡ç†ï¼ˆå¯é€‰ï¼‰
~~~

**ä½¿ç”¨åœºæ™¯:**
- `name`: ä¼šå‡ºç°åœ¨æ—¥å¿—æ–‡ä»¶åä¸­ï¼ˆ`log_mnist_baseline_20251103.log`ï¼‰
- `seed`: è®¾ç½® Python/NumPy/PyTorch çš„å…¨å±€éšæœºç§å­
- `tags`: ç”¨äºå®éªŒç®¡ç†å·¥å…·ï¼ˆå¦‚ MLflow, Weights & Biasesï¼‰

### 2. æ•°æ®é›†é…ç½® (dataset)

~~~yaml
dataset:
  name: "MNIST"           # æ”¯æŒ: MNIST, FashionMNIST, CIFAR10, CIFAR100
  data_path: "./data"     # æ•°æ®å­˜å‚¨è·¯å¾„
~~~

**æ·»åŠ è‡ªå®šä¹‰æ•°æ®é›†:**

åœ¨ `utils/data.py` çš„ `_DATASET_REGISTRY` ä¸­æ³¨å†Œï¼š

~~~python
_DATASET_REGISTRY['MyDataset'] = {
    'torchvision_class': datasets.MyDataset,
    'im_size': (224, 224),
    'channel': 3,
    'num_classes': 100,
    'mean': [0.5, 0.5, 0.5],
    'std': [0.5, 0.5, 0.5],
}
~~~

### 3. æ•°æ®åŠ è½½å™¨é…ç½® (dataloader)

~~~yaml
dataloader:
  batch_size: 128          # è®­ç»ƒæ‰¹æ¬¡å¤§å°
  eval_batch_size: 256     # è¯„ä¼°æ‰¹æ¬¡å¤§å°ï¼ˆé€šå¸¸æ˜¯è®­ç»ƒçš„2å€ï¼‰
  num_workers: 4           # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
  pin_memory: true         # æ˜¯å¦å›ºå®šå†…å­˜ï¼ˆGPUè®­ç»ƒå¿…å¼€ï¼‰
  persistent_workers: true # ä¿æŒå·¥ä½œè¿›ç¨‹å¸¸é©»
~~~

**æ€§èƒ½è°ƒä¼˜å»ºè®®:**

| ç¡¬ä»¶é…ç½® | num_workers | batch_size | pin_memory |
|---------|-------------|------------|------------|
| RTX 3060 (12GB) | 4 | 128 | true |
| RTX 4090 (24GB) | 8 | 256 | true |
| CPU Only | 0 | 64 | false |
| Windows ç³»ç»Ÿ | 0 | 128 | true |

### 4. è®­ç»ƒé…ç½® (training)

#### 4.1 åŸºç¡€é…ç½®

~~~yaml
training:
  epochs: 100              # è®­ç»ƒè½®æ•°
  lr: 0.001               # åˆå§‹å­¦ä¹ ç‡
  optimizer: "AdamW"      # ä¼˜åŒ–å™¨ç±»å‹
  criterion: "CrossEntropyLoss"  # æŸå¤±å‡½æ•°
~~~

**å¸¸ç”¨ä¼˜åŒ–å™¨é…ç½®:**

~~~yaml
# AdamW (æ¨è)
optimizer: "AdamW"
optimizer_params:
  weight_decay: 0.01
  betas: [0.9, 0.999]

# SGD with Momentum
optimizer: "SGD"
optimizer_params:
  momentum: 0.9
  nesterov: true
  weight_decay: 0.0001
~~~

#### 4.2 æ€§èƒ½ä¼˜åŒ–

~~~yaml
training:
  use_amp: true           # è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆæ¨èGPUè®­ç»ƒå¼€å¯ï¼‰
  grad_accum_steps: 2     # æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿæ›´å¤§batchï¼‰
  max_grad_norm: 1.0      # æ¢¯åº¦è£å‰ªï¼ˆRNN/Transformerå¿…éœ€ï¼‰
~~~

**æ˜¾å­˜ä¸è¶³è§£å†³æ–¹æ¡ˆ:**

| æ–¹æ³• | é…ç½® | æ˜¾å­˜èŠ‚çœ | æ€§èƒ½å½±å“ |
|------|------|---------|---------|
| å¯ç”¨ AMP | `use_amp: true` | ~30% | +10% é€Ÿåº¦ |
| æ¢¯åº¦ç´¯ç§¯ | `grad_accum_steps: 4` | ~75% | è½»å¾®å‡é€Ÿ |
| å‡å° batch | `batch_size: 64` | 50% | å¯èƒ½é™ä½ç²¾åº¦ |
| æ··åˆä½¿ç”¨ | ä»¥ä¸Šä¸‰è€…ç»“åˆ | ~80% | æ•´ä½“åŠ é€Ÿ |

#### 4.3 å­¦ä¹ ç‡è°ƒåº¦

~~~yaml
training:
  scheduler: "CosineAnnealingLR"
  scheduler_params:
    T_max: 100            # å‘¨æœŸé•¿åº¦ï¼ˆé€šå¸¸è®¾ä¸ºæ€»epochsï¼‰
    eta_min: 0.00001      # æœ€å°å­¦ä¹ ç‡
~~~

**å¸¸ç”¨è°ƒåº¦å™¨é…ç½®:**

~~~yaml
# 1. ä½™å¼¦é€€ç« (æ¨è)
scheduler: "CosineAnnealingLR"
scheduler_params:
  T_max: 100
  eta_min: 0.00001

# 2. é˜¶æ¢¯å¼ä¸‹é™
scheduler: "StepLR"
scheduler_params:
  step_size: 30          # æ¯30ä¸ªepoché™ä½ä¸€æ¬¡
  gamma: 0.1             # å­¦ä¹ ç‡å˜ä¸ºåŸæ¥çš„0.1å€

# 3. åŸºäºæŒ‡æ ‡çš„è‡ªé€‚åº”è°ƒæ•´
scheduler: "ReduceLROnPlateau"
scheduler_params:
  mode: 'max'            # è·Ÿè¸ªæŒ‡æ ‡çš„æ¨¡å¼
  factor: 0.1            # é™ä½å› å­
  patience: 10           # è€å¿ƒå€¼
  threshold: 0.0001      # æœ€å°æ”¹å–„é˜ˆå€¼

# 4. OneCycleLR (å¿«é€Ÿæ”¶æ•›)
scheduler: "OneCycleLR"
scheduler_params:
  max_lr: 0.01
  total_steps: 50000     # æ€»è®­ç»ƒæ­¥æ•°
  pct_start: 0.3         # ä¸Šå‡é˜¶æ®µå æ¯”
~~~

#### 4.4 æ—©åœé…ç½®

~~~yaml
training:
  patience: 10           # è¿ç»­10ä¸ªepochä¸æ”¹å–„å°±åœæ­¢
  min_delta: 0.001       # æœ€å°æ”¹å–„é˜ˆå€¼
  metric_to_track: "acc" # è·Ÿè¸ªçš„æŒ‡æ ‡
  metric_mode: "max"     # 'max'(å‡†ç¡®ç‡) æˆ– 'min'(æŸå¤±)
~~~

**æ—©åœç­–ç•¥å»ºè®®:**

| æ•°æ®é›†è§„æ¨¡ | patience | min_delta |
|-----------|----------|-----------|
| å°å‹ (<10k) | 5-10 | 0.01 |
| ä¸­å‹ (10k-100k) | 10-20 | 0.001 |
| å¤§å‹ (>100k) | 20-50 | 0.0001 |

### 5. æ£€æŸ¥ç‚¹é…ç½® (checkpoint)

~~~yaml
checkpoint:
  save_dir: "./checkpoints"  # å­˜å‚¨ç›®å½•
  max_to_keep: 3            # ä¿ç•™æœ€è¿‘3ä¸ªepochæ£€æŸ¥ç‚¹
  auto_resume: true         # è‡ªåŠ¨æ¢å¤è®­ç»ƒ
~~~

**æ£€æŸ¥ç‚¹æ–‡ä»¶è¯´æ˜:**

```
checkpoints/
â”œâ”€â”€ best_model.pth              # æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯æŒ‡æ ‡æœ€å¥½ï¼‰
â”œâ”€â”€ checkpoint_epoch_97.pth     # ç¬¬97ä¸ªepochçš„æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint_epoch_98.pth     # ç¬¬98ä¸ªepochçš„æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint_epoch_99.pth     # ç¬¬99ä¸ªepochçš„æ£€æŸ¥ç‚¹
â””â”€â”€ interrupt_checkpoint.pth    # ä¸­æ–­æ£€æŸ¥ç‚¹ï¼ˆCtrl+Cåä¿å­˜ï¼‰
```

### 6. Ntfy é€šçŸ¥é…ç½® (ntfy)

~~~yaml
ntfy:
  enabled: true                          # æ˜¯å¦å¯ç”¨é€šçŸ¥
  server_url: "https://ntfy.sh"          # æœåŠ¡å™¨åœ°å€
  topic: "your_unique_topic_name_here"   # é€šçŸ¥ä¸»é¢˜ï¼ˆéœ€ä¿®æ”¹ï¼‰
~~~

**è®¾ç½®æ­¥éª¤:**

1. æ‰‹æœºå®‰è£… Ntfy Appï¼ˆiOS/Androidï¼‰
2. ä¿®æ”¹ `topic` ä¸ºä½ çš„å”¯ä¸€åç§°ï¼ˆä¾‹å¦‚: `trouvaille_ml_project_xyz123`ï¼‰
3. åœ¨ App ä¸­è®¢é˜…ç›¸åŒçš„ topic
4. å¼€å§‹è®­ç»ƒï¼Œä½ ä¼šæ”¶åˆ°é€šçŸ¥ï¼

**é€šçŸ¥ç¤ºä¾‹:**

- ğŸƒ **è®­ç»ƒå¼€å§‹**: "è®­ç»ƒå¼€å§‹ Epochs: 1 â†’ 100"
- âœ… **è®­ç»ƒæˆåŠŸ**: "è®­ç»ƒå·²æ­£å¸¸å®Œæˆ æ€»è½®æ•°: 100 æœ€ä½³æŒ‡æ ‡: 0.9845"
- âŒ **è®­ç»ƒå¤±è´¥**: "è®­ç»ƒå¤±è´¥: RuntimeError ..."

---

## å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: å¿«é€Ÿå®éªŒï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰

**config.yaml:**
~~~yaml
experiment:
  name: "quick_test"

training:
  epochs: 10
  lr: 0.001

checkpoint:
  save_dir: "./test_checkpoints"

ntfy:
  enabled: false
~~~

### åœºæ™¯ 2: é«˜æ€§èƒ½è®­ç»ƒï¼ˆå¤§batch + AMPï¼‰

**config.yaml:**
~~~yaml
dataloader:
  batch_size: 256
  num_workers: 8
  pin_memory: true

training:
  use_amp: true
  grad_accum_steps: 1
  optimizer: "AdamW"
  scheduler: "OneCycleLR"
  scheduler_params:
    max_lr: 0.01
    total_steps: 50000

advanced:
  cudnn_benchmark: true
~~~

### åœºæ™¯ 3: æ˜¾å­˜å—é™ï¼ˆå°batch + æ¢¯åº¦ç´¯ç§¯ï¼‰

**config.yaml:**
~~~yaml
dataloader:
  batch_size: 32

training:
  use_amp: true
  grad_accum_steps: 8  # æœ‰æ•ˆbatch = 32 Ã— 8 = 256
  max_grad_norm: 1.0

advanced:
  gradient_checkpointing: true
~~~

### åœºæ™¯ 4: è°ƒè¯•æ¨¡å¼

**config.yaml:**
~~~yaml
training:
  epochs: 5
  log_interval: 1

logging:
  console_level: "DEBUG"
  file_level: "DEBUG"

advanced:
  anomaly_detection: true
  deterministic: true

ntfy:
  enabled: false
~~~

### åœºæ™¯ 5: åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šGPUï¼‰

**config.yaml:**
~~~yaml
device:
  type: "cuda"
  multi_gpu:
    enabled: true
    device_ids: [0, 1, 2, 3]

dataloader:
  batch_size: 64  # æ¯ä¸ªGPUçš„batch size
  num_workers: 4  # æ¯ä¸ªGPUçš„workeræ•°

training:
  use_amp: true
~~~

---

## å‘½ä»¤è¡Œå‚æ•°è¦†ç›–

### åŸºç¡€ç”¨æ³•

~~~bash
# è¦†ç›–å•ä¸ªå‚æ•°
python main.py --training.epochs 200

# è¦†ç›–å¤šä¸ªå‚æ•°
python main.py \
  --training.epochs 200 \
  --training.lr 0.01 \
  --dataloader.batch_size 256

# æŒ‡å®šé…ç½®æ–‡ä»¶
python main.py --config my_experiment.yaml

# ç»„åˆä½¿ç”¨
python main.py \
  --config baseline.yaml \
  --training.epochs 50 \
  --experiment.name "baseline_v2"
~~~

### é«˜çº§ç”¨æ³•

~~~bash
# ç¦ç”¨æŸäº›åŠŸèƒ½
python main.py \
  --training.use_amp false \
  --ntfy.enabled false

# åˆ‡æ¢ä¼˜åŒ–å™¨
python main.py \
  --training.optimizer SGD \
  --training.optimizer_params.momentum 0.9

# ä¿®æ”¹åµŒå¥—é…ç½®
python main.py \
  --training.scheduler_params.T_max 50 \
  --training.scheduler_params.eta_min 0.00001
~~~

---

## é…ç½®éªŒè¯ä¸è°ƒè¯•

### 1. æ‰“å°å½“å‰é…ç½®

~~~python
from utils import print_config

config = setup_config(...)
print_config(config, title="å½“å‰ä½¿ç”¨çš„é…ç½®")
~~~

**è¾“å‡ºç¤ºä¾‹:**
```
============================================================
å½“å‰ä½¿ç”¨çš„é…ç½®
============================================================
experiment:
  name: mnist_baseline
  seed: 42
training:
  epochs: 100
  lr: 0.001
  ...
============================================================
```

### 2. ä¿å­˜è¿è¡Œæ—¶é…ç½®

~~~python
from utils import save_config_to_yaml

# åœ¨è®­ç»ƒå¼€å§‹å‰ä¿å­˜å®Œæ•´é…ç½®
save_config_to_yaml(config, './logs/run_config.yaml')
~~~

è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ¬¡è¿è¡Œéƒ½æœ‰é…ç½®çš„å®Œæ•´è®°å½•ã€‚

### 3. é…ç½®éªŒè¯

~~~python
def validate_config(config):
    """éªŒè¯é…ç½®çš„åˆæ³•æ€§"""
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    assert hasattr(config, 'training'), "ç¼ºå°‘ training é…ç½®"
    assert hasattr(config, 'dataset'), "ç¼ºå°‘ dataset é…ç½®"
    
    # æ£€æŸ¥å‚æ•°èŒƒå›´
    assert config.training.epochs > 0, "epochs å¿…é¡»å¤§äº 0"
    assert config.training.lr > 0, "lr å¿…é¡»å¤§äº 0"
    assert config.training.patience > 0, "patience å¿…é¡»å¤§äº 0"
    
    # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
    import os
    os.makedirs(config.checkpoint.save_dir, exist_ok=True)
    os.makedirs(config.logging.log_dir, exist_ok=True)
    
    logger.info("é…ç½®éªŒè¯é€šè¿‡")

# åœ¨ main.py ä¸­ä½¿ç”¨
config = setup_config(...)
validate_config(config)
~~~

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰é…ç½®é¡¹ï¼Ÿ

**A:** ç›´æ¥åœ¨ `config.yaml` ä¸­æ·»åŠ ï¼Œç„¶ååœ¨ä»£ç ä¸­è®¿é—®ï¼š

~~~yaml
# config.yaml
my_custom_config:
  param1: 100
  param2: "hello"
~~~

~~~python
# main.py
print(config.my_custom_config.param1)  # 100
print(config.my_custom_config.param2)  # "hello"
~~~

### Q2: é…ç½®æ–‡ä»¶å¤ªé•¿ï¼Œå¦‚ä½•ç»„ç»‡ï¼Ÿ

**A:** ä½¿ç”¨ YAML çš„é”šç‚¹å’Œå¼•ç”¨åŠŸèƒ½ï¼š

~~~yaml
# å®šä¹‰å…¬å…±é…ç½®
common_training: &common_training
  use_amp: true
  grad_accum_steps: 2

# å®éªŒ1
experiment1:
  training:
    <<: *common_training
    epochs: 100
    lr: 0.001

# å®éªŒ2
experiment2:
  training:
    <<: *common_training
    epochs: 200
    lr: 0.01
~~~

### Q3: å¦‚ä½•ç®¡ç†å¤šä¸ªå®éªŒçš„é…ç½®ï¼Ÿ

**A:** ä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºå•ç‹¬çš„é…ç½®æ–‡ä»¶ï¼š

```
configs/
â”œâ”€â”€ baseline.yaml
â”œâ”€â”€ experiment1_high_lr.yaml
â”œâ”€â”€ experiment2_large_batch.yaml
â””â”€â”€ experiment3_augmentation.yaml
```

è¿è¡Œæ—¶æŒ‡å®šï¼š
~~~bash
python main.py --config configs/experiment1_high_lr.yaml
~~~

### Q4: å‘½ä»¤è¡Œå‚æ•°ä¸ç”Ÿæ•ˆï¼Ÿ

**A:** æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š

1. å‚æ•°åæ˜¯å¦æ­£ç¡®ï¼ˆä½¿ç”¨ç‚¹åˆ†éš”ï¼‰
2. æ˜¯å¦åœ¨ `parse_arguments()` ä¸­å®šä¹‰
3. é…ç½®ä¼˜å…ˆçº§æ˜¯å¦æ­£ç¡®

~~~python
# argparse å®šä¹‰
parser.add_argument('--training.epochs', type=int)

# å‘½ä»¤è¡Œä½¿ç”¨
python main.py --training.epochs 100  # âœ… æ­£ç¡®
python main.py --epochs 100           # âŒ é”™è¯¯
~~~

---

## æœ€ä½³å®è·µ

1. **ç‰ˆæœ¬æ§åˆ¶**: å°† `config.yaml.example` æäº¤åˆ° gitï¼Œ`.gitignore` ä¸­æ’é™¤ `config.yaml`

2. **æ–‡æ¡£åŒ–**: åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ è¯¦ç»†æ³¨é‡Šï¼Œè¯´æ˜æ¯ä¸ªå‚æ•°çš„ä½œç”¨

3. **æ¨¡å—åŒ–**: å°†ä¸åŒç±»å‹çš„é…ç½®åˆ†ç»„ï¼Œä¿æŒç»“æ„æ¸…æ™°

4. **é»˜è®¤å€¼**: åœ¨ä»£ç ä¸­æä¾›åˆç†çš„é»˜è®¤å€¼ï¼Œé…ç½®æ–‡ä»¶åªè¦†ç›–éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†

5. **éªŒè¯**: åœ¨è®­ç»ƒå¼€å§‹å‰éªŒè¯é…ç½®çš„åˆæ³•æ€§

6. **è®°å½•**: æ¯æ¬¡è®­ç»ƒéƒ½ä¿å­˜å®Œæ•´çš„é…ç½®å¿«ç…§

7. **å®éªŒç®¡ç†**: ä½¿ç”¨æœ‰æ„ä¹‰çš„ `experiment.name` å’Œ `tags`

---

## æ€»ç»“

é…ç½®æ–‡ä»¶æ˜¯å®éªŒçš„"è“å›¾"ï¼Œåˆç†ä½¿ç”¨å¯ä»¥ï¼š

- âœ… æé«˜å®éªŒçš„å¯å¤ç°æ€§
- âœ… ç®€åŒ–å‚æ•°è°ƒä¼˜æµç¨‹
- âœ… ä¾¿äºå›¢é˜Ÿåä½œå’ŒçŸ¥è¯†å…±äº«
- âœ… æ”¯æŒå¿«é€Ÿåˆ‡æ¢å®éªŒé…ç½®

è®°ä½é…ç½®ä¼˜å…ˆçº§ï¼š**å‘½ä»¤è¡Œ > YAML > ä»£ç é»˜è®¤å€¼**