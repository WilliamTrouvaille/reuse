# `Trainer` ç±»å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼ˆæœ€ç»ˆç‰ˆï¼‰

## ğŸ“š ç›®å½•
1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [ä¸¤ç§åˆå§‹åŒ–æ¨¡å¼](#ä¸¤ç§åˆå§‹åŒ–æ¨¡å¼)
3. [è¿›åº¦æ¡æ§åˆ¶](#è¿›åº¦æ¡æ§åˆ¶)
4. [è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤](#è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤)
5. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
6. [å®Œæ•´ç¤ºä¾‹é¡¹ç›®](#å®Œæ•´ç¤ºä¾‹é¡¹ç›®)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€å°åŒ–ç¤ºä¾‹ï¼ˆ5 è¡Œä»£ç ï¼‰

```python
from utils import Trainer, setup_logging, get_device

setup_logging()
device = get_device('cuda')
model = YourModel().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

# åˆ›å»º Trainer å¹¶å¼€å§‹è®­ç»ƒ
trainer = Trainer(model, optimizer, criterion, device, use_amp=True)
history = trainer.fit(train_loader, val_loader, epochs=100)
```

å°±è¿™ä¹ˆç®€å•ï¼`Trainer` ä¼šè‡ªåŠ¨å¤„ç†ï¼š
- âœ… è®­ç»ƒ/éªŒè¯å¾ªç¯
- âœ… é«˜æ€§èƒ½æŒ‡æ ‡è·Ÿè¸ªï¼ˆGPU ç´¯ç§¯ï¼‰
- âœ… è¿›åº¦æ¡æ˜¾ç¤ºï¼ˆå¯é€‰ï¼‰
- âœ… æ—¥å¿—è¾“å‡º
- âœ… å†…å­˜ç®¡ç†

---

## ğŸ“ ä¸¤ç§åˆå§‹åŒ–æ¨¡å¼

### æ¨¡å¼ 1: ä¾èµ–æ³¨å…¥æ¨¡å¼ï¼ˆæ¨èï¼Œå®Œå…¨æ§åˆ¶ï¼‰

é€‚ç”¨åœºæ™¯ï¼šç ”ç©¶ä»£ç ã€éœ€è¦çµæ´»é…ç½®

```python
from utils import Trainer, CheckpointManager, EarlyStopper, NtfyNotifier

# 1. å‡†å¤‡æ ¸å¿ƒç»„ä»¶
device = get_device('cuda')
model = ResNet18(num_classes=10).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

# 2. å‡†å¤‡å¯é€‰å·¥å…·
ckpt_manager = CheckpointManager('./checkpoints', device=device, max_to_keep=3)
early_stopper = EarlyStopper(patience=10, mode='max', delta=0.001)
notifier = NtfyNotifier()

# 3. åˆ›å»º Trainerï¼ˆæ³¨å…¥æ‰€æœ‰å·¥å…·ï¼‰
trainer = Trainer(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    # æ³¨å…¥å·¥å…·
    checkpoint_manager=ckpt_manager,
    early_stopper=early_stopper,
    notifier=notifier,
    # æ€§èƒ½ä¼˜åŒ–
    use_amp=True,
    grad_accum_steps=1,
    max_grad_norm=1.0,
    # æŒ‡æ ‡ä¸æ—¥å¿—
    metric_to_track='acc',
    metric_mode='max',
    compute_top5=False,
    log_interval=1,
    val_interval=1,
    # è¿›åº¦æ¡æ§åˆ¶
    show_progress=True,
    progress_update_interval=0.5
)

# 4. å¼€å§‹è®­ç»ƒ
history = trainer.fit(train_loader, val_loader, epochs=100)
```

**ä¼˜åŠ¿**:
- âœ… å®Œå…¨æ§åˆ¶æ‰€æœ‰ç»„ä»¶çš„åˆ›å»ºå’Œé…ç½®
- âœ… æ˜“äºæµ‹è¯•ï¼ˆå¯ä»¥æ³¨å…¥ mock å¯¹è±¡ï¼‰
- âœ… é€‚åˆå¤æ‚çš„ç ”ç©¶é¡¹ç›®

### æ¨¡å¼ 2: é…ç½®é©±åŠ¨æ¨¡å¼ï¼ˆç®€åŒ–ï¼Œé€‚åˆæ ‡å‡†æµç¨‹ï¼‰

é€‚ç”¨åœºæ™¯ï¼šç”Ÿäº§ç¯å¢ƒã€æ ‡å‡†è®­ç»ƒæµç¨‹

```python
from utils import Trainer, setup_config

# 1. åŠ è½½é…ç½®
config = setup_config(
    default_config=DEFAULT_CONFIG,
    yaml_config_path='config.yaml',
    cmd_args=vars(args)
)

# 2. å‡†å¤‡æ ¸å¿ƒç»„ä»¶
device = get_device(config.device)
model = create_model(config).to(device)
optimizer = create_optimizer(model, config)
criterion = create_criterion(config)

# 3. ä½¿ç”¨ from_config åˆ›å»º Trainer
trainer = Trainer.from_config(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    config=config  # ä¼ å…¥å®Œæ•´é…ç½®å¯¹è±¡
)

# 4. å¼€å§‹è®­ç»ƒ
history = trainer.fit(train_loader, val_loader)
```

**é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼ˆconfig.yamlï¼‰**:
```yaml
training:
  epochs: 100
  use_amp: true
  grad_accum_steps: 1
  max_grad_norm: 1.0
  patience: 10
  min_delta: 0.001
  metric_to_track: 'acc'
  metric_mode: 'max'
  compute_top5: false
  log_interval: 1
  val_interval: 1
  show_progress: true
  progress_update_interval: 0.5

checkpoint:
  enabled: true
  save_dir: './checkpoints'
  max_to_keep: 3

ntfy:
  enabled: true
```

**ä¼˜åŠ¿**:
- âœ… é…ç½®ä¸ä»£ç åˆ†ç¦»ï¼Œæ˜“äºç®¡ç†
- âœ… è‡ªåŠ¨å®ä¾‹åŒ–æ‰€æœ‰å·¥å…·
- âœ… é€‚åˆæ ‡å‡†è®­ç»ƒæµç¨‹

---

## ğŸ¬ è¿›åº¦æ¡æ§åˆ¶

### åœºæ™¯ 1: å¯ç”¨è¿›åº¦æ¡ï¼ˆé»˜è®¤ï¼‰

```python
trainer = Trainer(
    model, optimizer, criterion, device,
    show_progress=True,  # é»˜è®¤å€¼
    progress_update_interval=0.5  # æ¯ 0.5 ç§’æ›´æ–°ä¸€æ¬¡
)

trainer.fit(train_loader, val_loader, epochs=100)
```

**è¾“å‡º**:
```
Epoch 1 [Train] |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:15<00:00, 12.8it/s, loss=0.5234, lr=1.0e-03]
Epoch 1 [Val]   |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:02<00:00, 18.3it/s, loss=0.4123]
Epoch 001 | Time: 17.2s | Train Loss: 0.5234 | Train Acc: 82.50% | Val Loss: 0.4123 | Val Acc: 85.20% | LR: 1.0e-03
```

### åœºæ™¯ 2: ç¦ç”¨è¿›åº¦æ¡ï¼ˆæœåŠ¡å™¨/è„šæœ¬æ¨¡å¼ï¼‰

é€‚ç”¨äºï¼š
- âŒ åå°è¿è¡Œ
- âŒ å†™å…¥æ—¥å¿—æ–‡ä»¶
- âŒ åœ¨éäº¤äº’å¼ç¯å¢ƒä¸­è¿è¡Œ

```python
trainer = Trainer(
    model, optimizer, criterion, device,
    show_progress=False  # ç¦ç”¨è¿›åº¦æ¡
)

trainer.fit(train_loader, val_loader, epochs=100)
```

**è¾“å‡º**ï¼ˆåªæœ‰ epoch æ€»ç»“ï¼‰:
```
Epoch 001 | Time: 17.2s | Train Loss: 0.5234 | Train Acc: 82.50% | Val Loss: 0.4123 | Val Acc: 85.20% | LR: 1.0e-03
Epoch 002 | Time: 16.8s | Train Loss: 0.4123 | Train Acc: 85.30% | Val Loss: 0.3821 | Val Acc: 87.10% | LR: 1.0e-03
```

### åœºæ™¯ 3: è°ƒæ•´è¿›åº¦æ¡æ›´æ–°é¢‘ç‡

```python
# æ›´æ–°é¢‘ç‡æ›´é«˜ï¼ˆæ›´å¹³æ»‘ï¼Œä½†å¯èƒ½å½±å“æ€§èƒ½ï¼‰
trainer = Trainer(
    model, optimizer, criterion, device,
    show_progress=True,
    progress_update_interval=0.1  # æ¯ 0.1 ç§’æ›´æ–°
)

# æ›´æ–°é¢‘ç‡æ›´ä½ï¼ˆèŠ‚çœ I/Oï¼Œæ¨èç”¨äºå¿«é€Ÿ GPUï¼‰
trainer = Trainer(
    model, optimizer, criterion, device,
    show_progress=True,
    progress_update_interval=2.0  # æ¯ 2 ç§’æ›´æ–°
)
```

**æ¨èè®¾ç½®**:
- æ…¢é€Ÿè®­ç»ƒï¼ˆCPU/å°æ¨¡å‹ï¼‰: `0.5s` - `1.0s`
- å¿«é€Ÿè®­ç»ƒï¼ˆGPU/å¤§æ¨¡å‹ï¼‰: `1.0s` - `2.0s`
- è¶…å¿«è®­ç»ƒï¼ˆå¤š GPUï¼‰: `2.0s` - `5.0s`

---

## ğŸ¨ è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤

### åœºæ™¯ 4: å¤šä»»åŠ¡å­¦ä¹ ï¼ˆåˆ†ç±» + åˆ†å‰²ï¼‰

```python
from utils import Trainer

class MultiTaskTrainer(Trainer):
    """å¤šä»»åŠ¡è®­ç»ƒå™¨ï¼šåŒæ—¶è®­ç»ƒåˆ†ç±»å’Œåˆ†å‰²"""
    
    def __init__(self, model, optimizer, criterion_cls, criterion_seg, device, **kwargs):
        # æ³¨æ„ï¼šä¸ä¼ å…¥ criterionï¼Œæˆ‘ä»¬è‡ªå·±ç®¡ç†å¤šä¸ªæŸå¤±å‡½æ•°
        super().__init__(
            model=model,
            optimizer=optimizer,
            criterion=None,  # ä¸ä½¿ç”¨
            device=device,
            **kwargs
        )
        self.criterion_cls = criterion_cls
        self.criterion_seg = criterion_seg
    
    def _train_step(self, batch):
        """é‡å†™è®­ç»ƒæ­¥éª¤ä»¥æ”¯æŒå¤šä»»åŠ¡"""
        # è§£åŒ…å¤šä»»åŠ¡æ•°æ®
        inputs, target_cls, target_seg = batch
        
        inputs = inputs.to(self.device, non_blocking=True)
        target_cls = target_cls.to(self.device, non_blocking=True)
        target_seg = target_seg.to(self.device, non_blocking=True)
        
        # å‰å‘ä¼ æ’­ï¼ˆå¤šä¸ªè¾“å‡ºï¼‰
        with autocast(device_type=self.device.type, enabled=(self.scaler is not None)):
            out_cls, out_seg = self.model(inputs)
            
            # è®¡ç®—å¤šä¸ªæŸå¤±
            loss_cls = self.criterion_cls(out_cls, target_cls)
            loss_seg = self.criterion_seg(out_seg, target_seg)
            
            # åŠ æƒç»„åˆ
            total_loss = loss_cls + 0.5 * loss_seg
        
        return {
            'loss': total_loss,
            'outputs': out_cls,     # ç”¨äºè®¡ç®—å‡†ç¡®ç‡
            'targets': target_cls
        }
    
    def _eval_step(self, batch):
        """é‡å†™è¯„ä¼°æ­¥éª¤"""
        return self._train_step(batch)

# ä½¿ç”¨
trainer = MultiTaskTrainer(
    model=multi_task_model,
    optimizer=optimizer,
    criterion_cls=nn.CrossEntropyLoss(),
    criterion_seg=nn.BCEWithLogitsLoss(),
    device=device,
    use_amp=True,
    show_progress=True
)

trainer.fit(train_loader, val_loader, epochs=100)
```

### åœºæ™¯ 5: å¯¹æ¯”å­¦ä¹ ï¼ˆSimCLRï¼‰

```python
from utils import Trainer
import torch.nn.functional as F

class ContrastiveTrainer(Trainer):
    """å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨ï¼ˆSimCLR é£æ ¼ï¼‰"""
    
    def __init__(self, model, optimizer, device, temperature=0.5, **kwargs):
        super().__init__(
            model=model,
            optimizer=optimizer,
            criterion=None,  # å¯¹æ¯”å­¦ä¹ ä¸éœ€è¦ä¼ ç»ŸæŸå¤±
            device=device,
            **kwargs
        )
        self.temperature = temperature
    
    def _train_step(self, batch):
        """é‡å†™è®­ç»ƒæ­¥éª¤ä»¥è®¡ç®—å¯¹æ¯”æŸå¤±"""
        # SimCLR: batch åŒ…å«ä¸¤ä¸ªå¢å¼ºè§†å›¾
        (view1, view2), _ = batch
        
        view1 = view1.to(self.device, non_blocking=True)
        view2 = view2.to(self.device, non_blocking=True)
        
        with autocast(device_type=self.device.type, enabled=(self.scaler is not None)):
            # è·å–åµŒå…¥
            z1 = self.model(view1)
            z2 = self.model(view2)
            
            # å½’ä¸€åŒ–
            z1 = F.normalize(z1, dim=1)
            z2 = F.normalize(z2, dim=1)
            
            # è®¡ç®—å¯¹æ¯”æŸå¤±
            batch_size = z1.size(0)
            z = torch.cat([z1, z2], dim=0)
            
            sim_matrix = torch.mm(z, z.T) / self.temperature
            labels = torch.arange(batch_size, device=self.device)
            labels = torch.cat([labels + batch_size, labels], dim=0)
            
            loss = F.cross_entropy(sim_matrix, labels)
        
        return {
            'loss': loss,
            'outputs': sim_matrix[:batch_size],
            'targets': labels[:batch_size]
        }

# ä½¿ç”¨
trainer = ContrastiveTrainer(
    model=simclr_model,
    optimizer=optimizer,
    device=device,
    temperature=0.5,
    use_amp=True,
    show_progress=False  # å¯¹æ¯”å­¦ä¹ é€šå¸¸è¾ƒå¿«ï¼Œå¯ä»¥ç¦ç”¨è¿›åº¦æ¡
)

trainer.fit(contrastive_loader, None, epochs=200)
```

### åœºæ™¯ 6: è‡ªå®šä¹‰é’©å­ï¼ˆé›†æˆ Weights & Biasesï¼‰

```python
from utils import Trainer
import wandb

class WandbTrainer(Trainer):
    """é›†æˆ W&B æ—¥å¿—çš„è®­ç»ƒå™¨"""
    
    def __init__(self, *args, wandb_project='my-project', **kwargs):
        super().__init__(*args, **kwargs)
        
        # åˆå§‹åŒ– wandb
        wandb.init(project=wandb_project)
        wandb.watch(self.model)
    
    def _on_train_epoch_end(self, epoch, train_metrics):
        """è®­ç»ƒç»“æŸæ—¶è®°å½•åˆ° wandb"""
        wandb.log({
            'epoch': epoch,
            'train/loss': train_metrics['loss'],
            'train/acc': train_metrics['acc'],
            'lr': train_metrics.get('lr', 0)
        })
    
    def _on_eval_epoch_end(self, epoch, val_metrics):
        """éªŒè¯ç»“æŸæ—¶è®°å½•åˆ° wandb"""
        wandb.log({
            'epoch': epoch,
            'val/loss': val_metrics['loss'],
            'val/acc': val_metrics['acc']
        })

# ä½¿ç”¨
trainer = WandbTrainer(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    wandb_project='cifar10-resnet',
    show_progress=True
)
```

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§ Batch Sizeï¼‰

```python
# æ˜¾å­˜åªæœ‰ 8GBï¼Œä½†æƒ³è¦ batch_size=512 çš„æ•ˆæœ
# æ–¹æ¡ˆï¼šbatch_size=128 + grad_accum_steps=4

trainer = Trainer(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    grad_accum_steps=4,  # ç´¯ç§¯ 4 æ­¥å†æ›´æ–°
    use_amp=True,         # è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
    show_progress=True
)

# ç­‰æ•ˆäº batch_size=512ï¼Œä½†åªç”¨ batch_size=128 çš„æ˜¾å­˜
train_loader = DataLoader(dataset, batch_size=128, ...)
trainer.fit(train_loader, val_loader, epochs=100)
```

### æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰

```python
trainer = Trainer(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    max_grad_norm=1.0,  # è£å‰ªæ¢¯åº¦èŒƒæ•°åˆ° 1.0
    show_progress=True
)
```

### å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆè‡ªåŠ¨å…¼å®¹ï¼‰

```python
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau

# 1. CosineAnnealingLR: ä½™å¼¦é€€ç«
scheduler = CosineAnnealingLR(optimizer, T_max=100)

# 2. ReduceLROnPlateau: æŒ‡æ ‡ä¸æ”¹å–„æ—¶è¡°å‡ï¼ˆè‡ªåŠ¨ä½¿ç”¨éªŒè¯æŒ‡æ ‡ï¼‰
scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=5)

# Trainer ä¼šè‡ªåŠ¨è¯†åˆ« scheduler ç±»å‹å¹¶æ­£ç¡®è°ƒç”¨
trainer = Trainer(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    scheduler=scheduler,
    metric_to_track='acc',  # ReduceLROnPlateau ä¼šä½¿ç”¨è¿™ä¸ªæŒ‡æ ‡
    show_progress=True
)
```

### Ntfy é€šçŸ¥ï¼ˆè®­ç»ƒçŠ¶æ€å®æ—¶æ¨é€ï¼‰

```python
from utils import Trainer, NtfyNotifier

notifier = NtfyNotifier()

trainer = Trainer(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    notifier=notifier,  # æ³¨å…¥é€šçŸ¥å™¨
    show_progress=True
)

# è®­ç»ƒå¼€å§‹ã€æˆåŠŸã€å¤±è´¥æ—¶ä¼šè‡ªåŠ¨å‘é€é€šçŸ¥åˆ°æ‰‹æœº
trainer.fit(train_loader, val_loader, epochs=100)
```

---

## ğŸ“¦ å®Œæ•´ç¤ºä¾‹é¡¹ç›®

### `main.py` - CIFAR-10 å®Œæ•´è®­ç»ƒè„šæœ¬

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CIFAR-10 è®­ç»ƒè„šæœ¬ï¼ˆä½¿ç”¨ utils.Trainerï¼‰
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torchvision.models import resnet18

from utils import (
    setup_logging,
    get_device,
    set_random_seed,
    Trainer,
    CheckpointManager,
    EarlyStopper,
    NtfyNotifier,
    save_dict_to_json
)


def get_dataloaders(batch_size=256, num_workers=4):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    train_dataset = datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    test_dataset = datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True
    )
    
    return train_loader, test_loader


def main():
    # ========== 1. é…ç½® ==========
    setup_logging(log_dir='./logs', console_level='INFO', file_level='DEBUG')
    set_random_seed(42)
    device = get_device('cuda')
    
    # ========== 2. æ•°æ® ==========
    train_loader, test_loader = get_dataloaders(batch_size=256, num_workers=4)
    
    # ========== 3. æ¨¡å‹ ==========
    model = resnet18(num_classes=10).to(device)
    
    # (å¯é€‰) ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
    if hasattr(torch, 'compile'):
        model = torch.compile(model)
    
    # ========== 4. ä¼˜åŒ–å™¨å’ŒæŸå¤± ==========
    optimizer = torch.optim.SGD(
        model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4
    )
    criterion = nn.CrossEntropyLoss()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)
    
    # ========== 5. å·¥å…· ==========
    ckpt_manager = CheckpointManager('./checkpoints', device=device, max_to_keep=3)
    early_stopper = EarlyStopper(patience=20, mode='max', delta=0.001)
    notifier = NtfyNotifier()
    
    # ========== 6. è®­ç»ƒå™¨ ==========
    trainer = Trainer(
        model=model,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        checkpoint_manager=ckpt_manager,
        early_stopper=early_stopper,
        notifier=notifier,
        scheduler=scheduler,
        use_amp=True,
        grad_accum_steps=1,
        max_grad_norm=None,
        metric_to_track='acc',
        metric_mode='max',
        compute_top5=False,
        log_interval=1,
        val_interval=1,
        show_progress=True,  # å¯ç”¨è¿›åº¦æ¡
        progress_update_interval=0.5
    )
    
    # ========== 7. å¼€å§‹è®­ç»ƒ ==========
    try:
        result = trainer.fit(
            train_loader=train_loader,
            val_loader=test_loader,
            epochs=200
        )
        
        # ä¿å­˜è®­ç»ƒå†å²
        save_dict_to_json(result, './training_history.json')
        
    except KeyboardInterrupt:
        notifier.notify_error("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­", "Ctrl+C")
    except Exception as e:
        notifier.notify_error("è®­ç»ƒå¤±è´¥", str(e))
        raise


if __name__ == '__main__':
    main()
```

**è¿è¡Œ**:
```bash
python main.py
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ç¦ç”¨è¿›åº¦æ¡ï¼Ÿ

**A**: è®¾ç½® `show_progress=False`:
```python
trainer = Trainer(..., show_progress=False)
```

### Q2: è¿›åº¦æ¡æ›´æ–°å¤ªé¢‘ç¹ï¼Œå½±å“æ€§èƒ½æ€ä¹ˆåŠï¼Ÿ

**A**: å¢å¤§ `progress_update_interval`:
```python
trainer = Trainer(..., progress_update_interval=2.0)  # æ¯ 2 ç§’æ›´æ–°
```

### Q3: æˆ‘çš„è®­ç»ƒé€»è¾‘å¾ˆç‰¹æ®Šï¼ŒTrainer èƒ½é€‚é…å—ï¼Ÿ

**A**: å¯ä»¥ï¼é€šè¿‡ç»§æ‰¿å¹¶é‡å†™ `_train_step()` æˆ– `_eval_step()` å³å¯ã€‚

### Q4: æ”¯æŒå¤š GPU è®­ç»ƒå—ï¼Ÿ

**A**: æ”¯æŒï¼åœ¨ä¼ å…¥ model ä¹‹å‰ç”¨ `DataParallel` åŒ…è£…ï¼š
```python
model = nn.DataParallel(model)
trainer = Trainer(model, ...)
```

### Q5: å¦‚ä½•æŸ¥çœ‹è®­ç»ƒå†å²ï¼Ÿ

**A**:
```python
result = trainer.fit(...)
history = result['history']
best_metric = result['best_metric']
```

### Q6: é…ç½®é©±åŠ¨æ¨¡å¼å’Œä¾èµ–æ³¨å…¥æ¨¡å¼å“ªä¸ªæ›´å¥½ï¼Ÿ

**A**:
- **ä¾èµ–æ³¨å…¥æ¨¡å¼**ï¼šç ”ç©¶ä»£ç ã€éœ€è¦çµæ´»é…ç½® â†’ æ¨è
- **é…ç½®é©±åŠ¨æ¨¡å¼**ï¼šç”Ÿäº§ç¯å¢ƒã€æ ‡å‡†æµç¨‹ â†’ æ¨è

ä¸¤ç§æ¨¡å¼å¯ä»¥æ··ç”¨ï¼

---

## ğŸ¯ æ€§èƒ½å¯¹æ¯”

| é…ç½® | ååé‡ (samples/s) | ç›¸å¯¹åŠ é€Ÿ |
|------|-------------------|---------|
| åŸå§‹ä»£ç  | 6097 | 1.0x |
| + MetricTracker | 8621 | 1.41x |
| + AMP | 13889 | **2.28x** |
| + ç¦ç”¨è¿›åº¦æ¡ | 14200 | **2.33x** |

**ç»“è®º**: `Trainer` + AMP + åˆç†é…ç½®å¯è¾¾ **2-2.5å€** åŠ é€Ÿï¼

---

## ğŸ“š æ€»ç»“

`Trainer` ç±»çš„æœ€ç»ˆè®¾è®¡ï¼š
1. **ä¸¤ç§åˆå§‹åŒ–æ¨¡å¼**ï¼šä¾èµ–æ³¨å…¥ï¼ˆçµæ´»ï¼‰ + é…ç½®é©±åŠ¨ï¼ˆç®€åŒ–ï¼‰
2. **è¿›åº¦æ¡å¯æ§**ï¼š`show_progress` å‚æ•°ï¼Œé€‚åº”ä¸åŒåœºæ™¯
3. **é«˜åº¦å¯å®šåˆ¶**ï¼š4 ä¸ªé’©å­æ–¹æ³• + 2 ä¸ªæ¨¡æ¿æ–¹æ³•
4. **æ€§èƒ½ä¼˜åŒ–åˆ°ä½**ï¼šAMP + MetricTracker + Progress
5. **å¥å£®æ€§å¼º**ï¼šè‡ªåŠ¨æ£€æŸ¥ç‚¹ã€æ—©åœã€ä¸­æ–­å¤„ç†ã€é€šçŸ¥

äº«å—é«˜æ•ˆè®­ç»ƒå§ï¼ğŸš€
