# `helpers` é€šç”¨è¾…åŠ©å‡½æ•°ä½¿ç”¨æŒ‡å—

## ğŸ“š ç›®å½•
1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [åŠŸèƒ½æ¦‚è§ˆ](#åŠŸèƒ½æ¦‚è§ˆ)
3. [æ—¶é—´å·¥å…·](#æ—¶é—´å·¥å…·)
4. [éšæœºç§å­ä¸å¯å¤ç°æ€§](#éšæœºç§å­ä¸å¯å¤ç°æ€§)
5. [è®¾å¤‡ç®¡ç†](#è®¾å¤‡ç®¡ç†)
6. [å†…å­˜ç®¡ç†](#å†…å­˜ç®¡ç†)
7. [æ¨¡å‹ä¸å¼ é‡å·¥å…·](#æ¨¡å‹ä¸å¼ é‡å·¥å…·)
8. [æ ¼å¼åŒ–ä¸ IO](#æ ¼å¼åŒ–ä¸-io)
9. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
from utils import (
    set_random_seed,
    get_device,
    count_parameters,
    log_memory_usage
)
from loguru import logger

# 1. è®¾ç½®éšæœºç§å­
set_random_seed(seed=42)

# 2. è·å–è®¡ç®—è®¾å¤‡
device = get_device('auto')  # è‡ªåŠ¨é€‰æ‹© GPU æˆ– CPU

# 3. åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ°è®¾å¤‡
model = YourModel().to(device)

# 4. ç»Ÿè®¡æ¨¡å‹å‚æ•°
total_params = count_parameters(model)
trainable_params = count_parameters(model, trainable_only=True)
logger.info(f"æ€»å‚æ•°: {total_params:,}")
logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

# 5. è®°å½•å†…å­˜ä½¿ç”¨
log_memory_usage("æ¨¡å‹åŠ è½½å")

# 6. å¼€å§‹è®­ç»ƒ...
```

---

## ğŸ“¦ åŠŸèƒ½æ¦‚è§ˆ

`helpers.py` æä¾›äº† **11 ä¸ªå®ç”¨å‡½æ•°**ï¼Œåˆ†ä¸º 5 å¤§ç±»ï¼š

| ç±»åˆ« | å‡½æ•° | ç”¨é€” |
|------|------|------|
| **æ—¶é—´å·¥å…·** | `get_time()` | è·å–æ ¼å¼åŒ–å½“å‰æ—¶é—´ |
| | `format_time()` | æ ¼å¼åŒ–ç§’æ•°ä¸ºå¯è¯»å­—ç¬¦ä¸² |
| **å¯å¤ç°æ€§** | `set_random_seed()` | è®¾ç½®å…¨å±€éšæœºç§å­ |
| **è®¾å¤‡ç®¡ç†** | `get_device()` | æ™ºèƒ½è®¾å¤‡é€‰æ‹©ä¸éªŒè¯ |
| | `clear_memory()` | æ¸…ç† GPU ç¼“å­˜ |
| | `get_memory_usage()` | è·å– GPU å†…å­˜ä½¿ç”¨æƒ…å†µ |
| | `log_memory_usage()` | è®°å½•å†…å­˜ä½¿ç”¨åˆ°æ—¥å¿— |
| **å¼ é‡ä¸æ¨¡å‹** | `validate_tensor()` | æ£€æŸ¥å¼ é‡æ˜¯å¦åŒ…å« NaN/Inf |
| | `count_parameters()` | ç»Ÿè®¡æ¨¡å‹å‚æ•°æ•°é‡ |
| **æ ¼å¼åŒ–ä¸ IO** | `format_size()` | æ ¼å¼åŒ–å­—èŠ‚å¤§å° |
| | `save_dict_to_json()` | ä¿å­˜å­—å…¸åˆ° JSON æ–‡ä»¶ |
| | `load_dict_from_json()` | ä» JSON æ–‡ä»¶åŠ è½½å­—å…¸ |

---

## â° æ—¶é—´å·¥å…·

### 1. `get_time()` - è·å–æ ¼å¼åŒ–å½“å‰æ—¶é—´

```python
from utils import get_time

# ä½¿ç”¨é»˜è®¤æ ¼å¼ "[%Y-%m-%d %H:%M:%S]"
current_time = get_time()
print(current_time)  # [2025-11-04 14:30:22]

# è‡ªå®šä¹‰æ ¼å¼
custom_time = get_time("%Y%m%d_%H%M%S")
print(custom_time)  # 20251104_143022
```

**å¸¸è§ç”¨é€”**ï¼š
- ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
- æ—¥å¿—è®°å½•
- è®­ç»ƒè¿›åº¦æ˜¾ç¤º

**ç¤ºä¾‹ï¼šç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ£€æŸ¥ç‚¹æ–‡ä»¶å**

```python
from utils import get_time

timestamp = get_time("%Y%m%d_%H%M%S")
checkpoint_path = f"checkpoints/model_{timestamp}.pth"
torch.save(model.state_dict(), checkpoint_path)
```

### 2. `format_time()` - æ ¼å¼åŒ–ç§’æ•°

```python
from utils import format_time

# å°äº 60 ç§’
print(format_time(45.5))      # 45.50s

# å°äº 1 å°æ—¶
print(format_time(135.7))     # 2m 15.7s

# å¤§äº 1 å°æ—¶
print(format_time(3725.3))    # 1h 2m 5.3s
```

**å¸¸è§ç”¨é€”**ï¼š
- æ˜¾ç¤ºè®­ç»ƒæ€»æ—¶é—´
- æ˜¾ç¤º Epoch å¹³å‡è€—æ—¶
- è¿›åº¦æ¡æ—¶é—´ä¼°è®¡

**ç¤ºä¾‹ï¼šè®­ç»ƒç»“æŸåæ˜¾ç¤ºæ€»è€—æ—¶**

```python
from utils import format_time
import time

start_time = time.time()

# ... è®­ç»ƒä»£ç  ...

total_time = time.time() - start_time
logger.success(f"è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {format_time(total_time)}")
```

---

## ğŸ² éšæœºç§å­ä¸å¯å¤ç°æ€§

### `set_random_seed()` - è®¾ç½®å…¨å±€éšæœºç§å­

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- è®¾ç½® Pythonã€NumPyã€PyTorch (CPU & GPU) çš„éšæœºç§å­
- ç¡®ä¿å®éªŒå¯å¤ç°
- æ”¯æŒ cuDNN benchmark æ€§èƒ½ä¼˜åŒ–

```python
from utils import set_random_seed

# åŸºç¡€ç”¨æ³•ï¼ˆä¿è¯å¯å¤ç°ï¼‰
set_random_seed(seed=42)

# æ€§èƒ½ä¼˜åŒ–æ¨¡å¼ï¼ˆç‰ºç‰²éƒ¨åˆ†å¯å¤ç°æ€§ï¼‰
set_random_seed(seed=42, enable_cudnn_benchmark=True)
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `seed` | int | 42 | éšæœºç§å­å€¼ |
| `enable_cudnn_benchmark` | bool | True | æ˜¯å¦å¯ç”¨ cuDNN è‡ªåŠ¨è°ƒä¼˜ |

### cuDNN Benchmark è¯¦è§£

**å¯ç”¨ benchmark (`enable_cudnn_benchmark=True`)**ï¼š
- âœ… åœ¨å›ºå®šè¾“å…¥å°ºå¯¸æ—¶å¯æå‡ **20-30% æ€§èƒ½**
- âŒ ä¼šç‰ºç‰²éƒ¨åˆ†å¯å¤ç°æ€§
- ğŸ“Œ é€‚ç”¨åœºæ™¯ï¼šç”Ÿäº§éƒ¨ç½²ã€å¤§è§„æ¨¡è®­ç»ƒ

**ç¦ç”¨ benchmark (`enable_cudnn_benchmark=False`)**ï¼š
- âœ… ä¿è¯ **å®Œå…¨å¯å¤ç°**
- âŒ æ€§èƒ½ç•¥ä½
- ğŸ“Œ é€‚ç”¨åœºæ™¯ï¼šè®ºæ–‡å¤ç°ã€æ¶ˆèå®éªŒ

### ä½¿ç”¨åœºæ™¯

**åœºæ™¯ 1: è®ºæ–‡å¤ç°ï¼ˆå®Œå…¨å¯å¤ç°ï¼‰**

```python
from utils import set_random_seed

# åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®
# experiment:
#   seed: 42
#   enable_cudnn_benchmark: false

set_random_seed(seed=42, enable_cudnn_benchmark=False)

# ç¡®ä¿ä¸¤æ¬¡è¿è¡Œç»“æœå®Œå…¨ä¸€è‡´
```

**åœºæ™¯ 2: ç”Ÿäº§è®­ç»ƒï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰**

```python
from utils import set_random_seed

# åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®
# experiment:
#   seed: 42
#   enable_cudnn_benchmark: true

set_random_seed(seed=42, enable_cudnn_benchmark=True)

# æå‡ 20-30% è®­ç»ƒé€Ÿåº¦
```

**åœºæ™¯ 3: ä¸é…ç½®æ–‡ä»¶é›†æˆ**

```python
from utils import setup_config, set_random_seed

config = setup_config(DEFAULT_CONFIG, 'config.yaml', {})

set_random_seed(
    seed=config.experiment.seed,
    enable_cudnn_benchmark=config.experiment.enable_cudnn_benchmark
)
```

---

## ğŸ–¥ï¸ è®¾å¤‡ç®¡ç†

### `get_device()` - æ™ºèƒ½è®¾å¤‡é€‰æ‹©

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- è‡ªåŠ¨é€‰æ‹© GPU æˆ– CPU
- å¤š GPU æ”¯æŒï¼ˆ`cuda:0`, `cuda:1` ç­‰ï¼‰
- æ™ºèƒ½å›é€€ï¼ˆè¯·æ±‚çš„ GPU ä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€ï¼‰

```python
from utils import get_device

# è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰
device = get_device('auto')  # æœ‰ GPU ç”¨ GPUï¼Œæ²¡æœ‰ç”¨ CPU

# æ‰‹åŠ¨æŒ‡å®š
device = get_device('cuda')      # ä½¿ç”¨ cuda:0
device = get_device('cuda:1')    # ä½¿ç”¨ç¬¬ 2 å— GPU
device = get_device('cpu')       # å¼ºåˆ¶ä½¿ç”¨ CPU
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•°å€¼ | è¡Œä¸º |
|--------|------|
| `'auto'` | è‡ªåŠ¨é€‰æ‹© GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ CPU |
| `'cpu'` | å¼ºåˆ¶ä½¿ç”¨ CPU |
| `'cuda'` | ä½¿ç”¨ `cuda:0`ï¼ˆå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ° CPUï¼‰ |
| `'cuda:N'` | ä½¿ç”¨ç¬¬ N å— GPUï¼ˆå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ° `cuda:0`ï¼‰ |

### æ™ºèƒ½å›é€€æœºåˆ¶

```python
from utils import get_device

# åœºæ™¯ 1: æ²¡æœ‰ GPU
device = get_device('cuda')
# è¾“å‡º: CUDA ä¸å¯ç”¨ã€‚å›é€€åˆ° CPUã€‚
# è¾“å‡º: è®¡ç®—è®¾å¤‡å·²è®¾ç½®ä¸º: CPU

# åœºæ™¯ 2: è¯·æ±‚çš„ GPU ä¸å­˜åœ¨
device = get_device('cuda:3')  # ä½†åªæœ‰ 2 å— GPU
# è¾“å‡º: è¯·æ±‚çš„ GPU cuda:3 ä¸å¯ç”¨ (ä»…æ‰¾åˆ° 2 å— GPU)ã€‚
# è¾“å‡º: å›é€€åˆ° 'cuda:0'ã€‚
# è¾“å‡º: è®¡ç®—è®¾å¤‡å·²è®¾ç½®ä¸º: cuda:0 (NVIDIA GeForce RTX 5070 Ti)
```

### ä½¿ç”¨åœºæ™¯

**åœºæ™¯ 1: å•æœºå•å¡è®­ç»ƒ**

```python
from utils import get_device

device = get_device('auto')
model = YourModel().to(device)

for images, labels in train_loader:
    images = images.to(device)
    labels = labels.to(device)
    # ... è®­ç»ƒä»£ç  ...
```

**åœºæ™¯ 2: å¤š GPU ç¯å¢ƒï¼ˆæŒ‡å®šå¡å·ï¼‰**

```python
from utils import get_device

# ä½¿ç”¨ç¬¬ 2 å— GPU (cuda:1)
device = get_device('cuda:1')
model = YourModel().to(device)
```

**åœºæ™¯ 3: ä¸é…ç½®æ–‡ä»¶é›†æˆ**

```yaml
# config.yaml
experiment:
  device: "auto"  # æˆ– "cuda:0", "cpu" ç­‰
```

```python
from utils import setup_config, get_device

config = setup_config(DEFAULT_CONFIG, 'config.yaml', {})
device = get_device(config.experiment.device)
```

---

## ğŸ’¾ å†…å­˜ç®¡ç†

### 1. `clear_memory()` - æ¸…ç† GPU ç¼“å­˜

```python
from utils import clear_memory

# åœ¨å¤§é‡ GPU æ“ä½œåæ¸…ç†ç¼“å­˜
clear_memory()
# è¾“å‡º: å·²æ¸…ç† GPU ç¼“å­˜ (torch.cuda.empty_cache())ã€‚
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- éªŒè¯/æµ‹è¯•åæ¸…ç†æ˜¾å­˜
- æ¨¡å‹åˆ‡æ¢æ—¶é‡Šæ”¾æ˜¾å­˜
- Out of Memory (OOM) é”™è¯¯é¢„é˜²

**ç¤ºä¾‹ï¼šéªŒè¯åæ¸…ç†æ˜¾å­˜**

```python
from utils import clear_memory

# è®­ç»ƒå®ŒæˆåéªŒè¯
model.eval()
with torch.no_grad():
    # ... éªŒè¯ä»£ç  ...
    pass

# æ¸…ç†éªŒè¯è¿‡ç¨‹ä¸­çš„ç¼“å­˜
clear_memory()
```

### 2. `get_memory_usage()` - è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ

```python
from utils import get_memory_usage

# ä½¿ç”¨é»˜è®¤è®¾å¤‡ï¼ˆå½“å‰è®¾å¤‡ï¼‰
usage = get_memory_usage()
print(usage)
# {'allocated': '2.5 GB', 'reserved': '3.0 GB', 'total': '12.0 GB', 'percent_used': '20.83%'}

# æŒ‡å®šè®¾å¤‡
usage = get_memory_usage(device=0)      # ä½¿ç”¨ GPU 0
usage = get_memory_usage(device=torch.device('cuda:1'))  # ä½¿ç”¨ GPU 1
```

**è¿”å›å€¼è¯´æ˜**ï¼š

| é”® | è¯´æ˜ |
|----|------|
| `allocated` | å½“å‰å·²åˆ†é…çš„æ˜¾å­˜ |
| `reserved` | PyTorch ç¼“å­˜æ± ä¸­çš„æ˜¾å­˜ |
| `total` | GPU æ€»æ˜¾å­˜ |
| `percent_used` | å·²ä½¿ç”¨ç™¾åˆ†æ¯” |

### 3. `log_memory_usage()` - è®°å½•å†…å­˜ä½¿ç”¨

```python
from utils import log_memory_usage

# åœ¨å…³é”®æ“ä½œå‰åè®°å½•
log_memory_usage("æ¨¡å‹åŠ è½½å‰")
model = YourModel().to(device)
log_memory_usage("æ¨¡å‹åŠ è½½å")

# è¾“å‡ºç¤ºä¾‹ï¼š
# æ¨¡å‹åŠ è½½å‰: 512.0 MB / 12.0 GB (4.17%)
# æ¨¡å‹åŠ è½½å: 2.5 GB / 12.0 GB (20.83%)
```

### å®Œæ•´ç¤ºä¾‹ï¼šå†…å­˜ç›‘æ§

```python
from utils import get_device, log_memory_usage, clear_memory
from loguru import logger

device = get_device('auto')

# 1. åˆå§‹çŠ¶æ€
log_memory_usage("åˆå§‹çŠ¶æ€")

# 2. åŠ è½½æ¨¡å‹
model = YourModel().to(device)
log_memory_usage("æ¨¡å‹åŠ è½½å")

# 3. è®­ç»ƒ
for epoch in range(100):
    # ... è®­ç»ƒä»£ç  ...

    # æ¯ 10 ä¸ª epoch æ£€æŸ¥ä¸€æ¬¡å†…å­˜
    if (epoch + 1) % 10 == 0:
        log_memory_usage(f"Epoch {epoch + 1}")

# 4. è®­ç»ƒç»“æŸåæ¸…ç†
del model
clear_memory()
log_memory_usage("æ¸…ç†å")
```

---

## ğŸ§ª æ¨¡å‹ä¸å¼ é‡å·¥å…·

### 1. `validate_tensor()` - éªŒè¯å¼ é‡æœ‰æ•ˆæ€§

**åŠŸèƒ½**ï¼šæ£€æŸ¥å¼ é‡æ˜¯å¦åŒ…å« NaN æˆ– Inf å€¼ã€‚

```python
from utils import validate_tensor
import torch

# æ­£å¸¸å¼ é‡
tensor = torch.randn(3, 3)
is_valid = validate_tensor(tensor, name="my_tensor")
# è¿”å›: True

# åŒ…å« NaN çš„å¼ é‡
tensor_nan = torch.tensor([1.0, float('nan'), 3.0])
is_valid = validate_tensor(tensor_nan, name="nan_tensor")
# è¾“å‡º: å¼ é‡ nan_tensor åŒ…å« NaN å€¼ï¼
# è¿”å›: False

# åŒ…å« Inf çš„å¼ é‡
tensor_inf = torch.tensor([1.0, float('inf'), 3.0])
is_valid = validate_tensor(tensor_inf, name="inf_tensor")
# è¾“å‡º: å¼ é‡ inf_tensor åŒ…å« Inf å€¼ï¼
# è¿”å›: False
```

**ä½¿ç”¨åœºæ™¯**ï¼š

**åœºæ™¯ 1: è°ƒè¯•è®­ç»ƒä¸ç¨³å®š**

```python
from utils import validate_tensor

for epoch in range(100):
    for images, labels in train_loader:
        # å‰å‘ä¼ æ’­
        outputs = model(images)

        # éªŒè¯è¾“å‡º
        if not validate_tensor(outputs, name="model_outputs"):
            logger.error(f"æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼Epoch {epoch}")
            break

        loss = criterion(outputs, labels)

        # éªŒè¯æŸå¤±
        if not validate_tensor(loss, name="loss"):
            logger.error(f"æŸå¤±å¼‚å¸¸ï¼Epoch {epoch}")
            break

        # åå‘ä¼ æ’­
        loss.backward()
```

**åœºæ™¯ 2: æ¢¯åº¦çˆ†ç‚¸æ£€æµ‹**

```python
from utils import validate_tensor

optimizer.step()

# æ£€æŸ¥æ¨¡å‹å‚æ•°
for name, param in model.named_parameters():
    if param.grad is not None:
        if not validate_tensor(param.grad, name=f"grad_{name}"):
            logger.error(f"æ¢¯åº¦çˆ†ç‚¸æ£€æµ‹ï¼å‚æ•°: {name}")
            break
```

### 2. `count_parameters()` - ç»Ÿè®¡æ¨¡å‹å‚æ•°

```python
from utils import count_parameters

model = YourModel()

# ç»Ÿè®¡æ‰€æœ‰å‚æ•°
total_params = count_parameters(model)
print(f"æ€»å‚æ•°: {total_params:,}")  # æ€»å‚æ•°: 1,234,567

# åªç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
trainable_params = count_parameters(model, trainable_only=True)
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")  # å¯è®­ç»ƒå‚æ•°: 1,234,567
```

**ä½¿ç”¨åœºæ™¯**ï¼š

**åœºæ™¯ 1: æ¨¡å‹ä¿¡æ¯ç»Ÿè®¡**

```python
from utils import count_parameters
from loguru import logger

model = YourModel()

total = count_parameters(model)
trainable = count_parameters(model, trainable_only=True)
frozen = total - trainable

logger.info("=" * 60)
logger.info("æ¨¡å‹å‚æ•°ç»Ÿè®¡".center(60))
logger.info("=" * 60)
logger.info(f"æ€»å‚æ•°: {total:,}")
logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable:,}")
logger.info(f"å†»ç»“å‚æ•°: {frozen:,}")
logger.info("=" * 60)
```

**åœºæ™¯ 2: è¿ç§»å­¦ä¹ ï¼ˆéƒ¨åˆ†å†»ç»“ï¼‰**

```python
from utils import count_parameters
from loguru import logger

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = torchvision.models.resnet18(pretrained=True)

# å†»ç»“ç‰¹å¾æå–å±‚
for param in model.parameters():
    param.requires_grad = False

# æ›¿æ¢åˆ†ç±»å¤´
model.fc = nn.Linear(512, num_classes)

# ç»Ÿè®¡å‚æ•°
total = count_parameters(model)
trainable = count_parameters(model, trainable_only=True)

logger.info(f"æ€»å‚æ•°: {total:,}")
logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable:,} (ä»…åˆ†ç±»å¤´)")
```

---

## ğŸ“ æ ¼å¼åŒ–ä¸ IO

### 1. `format_size()` - æ ¼å¼åŒ–å­—èŠ‚å¤§å°

```python
from utils import format_size

print(format_size(0))           # 0B
print(format_size(1024))        # 1.0 KB
print(format_size(1048576))     # 1.0 MB
print(format_size(1073741824))  # 1.0 GB
```

**ä½¿ç”¨åœºæ™¯**ï¼š

```python
from utils import format_size
import os

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤§å°
model_path = "checkpoints/best_model.pth"
file_size = os.path.getsize(model_path)
logger.info(f"æ¨¡å‹æ–‡ä»¶å¤§å°: {format_size(file_size)}")
```

### 2. `save_dict_to_json()` - ä¿å­˜å­—å…¸åˆ° JSON

```python
from utils import save_dict_to_json

data = {
    "experiment": "CIFAR10_ResNet18",
    "best_acc": 92.5,
    "epochs": 100,
    "batch_size": 128
}

save_dict_to_json(data, "./results/experiment_config.json")
# è¾“å‡º: æ•°æ®å·²ä¿å­˜åˆ° JSON æ–‡ä»¶: ./results/experiment_config.json
```

**ç‰¹ç‚¹**ï¼š
- âœ… è‡ªåŠ¨åˆ›å»ºç›®å½•
- âœ… UTF-8 ç¼–ç ï¼Œæ”¯æŒä¸­æ–‡
- âœ… æ ¼å¼åŒ–è¾“å‡ºï¼ˆç¼©è¿› 4 ç©ºæ ¼ï¼‰
- âœ… ä¿å­˜å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

### 3. `load_dict_from_json()` - ä» JSON åŠ è½½å­—å…¸

```python
from utils import load_dict_from_json

data = load_dict_from_json("./results/experiment_config.json")

if data is not None:
    print(data["experiment"])  # CIFAR10_ResNet18
    print(data["best_acc"])    # 92.5
```

**è¿”å›å€¼**ï¼š
- æˆåŠŸï¼šè¿”å›å­—å…¸
- å¤±è´¥ï¼šè¿”å› `None`

### å®Œæ•´ç¤ºä¾‹ï¼šä¿å­˜è®­ç»ƒç»“æœ

```python
from utils import save_dict_to_json, format_time
import time

# è®­ç»ƒè¿‡ç¨‹
start_time = time.time()
# ... è®­ç»ƒä»£ç  ...
total_time = time.time() - start_time

# ä¿å­˜ç»“æœ
results = {
    "dataset": "CIFAR10",
    "model": "ResNet18",
    "best_train_acc": 95.2,
    "best_val_acc": 92.8,
    "total_epochs": 100,
    "total_time": format_time(total_time),
    "batch_size": 128,
    "learning_rate": 0.01
}

save_dict_to_json(results, "./results/training_results.json")
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: `set_random_seed()` èƒ½ä¿è¯ 100% å¯å¤ç°å—ï¼Ÿ

**A**: å‡ ä¹å¯ä»¥ï¼Œä½†æœ‰ä»¥ä¸‹ä¾‹å¤–ï¼š

1. **cuDNN Benchmark å¯ç”¨æ—¶**ï¼š
   - è®¾ç½® `enable_cudnn_benchmark=False` å¯ä¿è¯å®Œå…¨å¯å¤ç°

2. **å¤šçº¿ç¨‹/å¤šè¿›ç¨‹æ•°æ®åŠ è½½**ï¼š
   - è®¾ç½® `DataLoader(num_workers=0)` å¯é¿å…

3. **ç¡¬ä»¶å·®å¼‚**ï¼š
   - ä¸åŒ GPU å‹å·å¯èƒ½æœ‰å¾®å°å·®å¼‚

**æ¨èé…ç½®ï¼ˆå®Œå…¨å¯å¤ç°ï¼‰**ï¼š

```yaml
# config.yaml
experiment:
  seed: 42
  enable_cudnn_benchmark: false

dataloader:
  num_workers: 0
```

### Q2: `get_device()` è¿”å›çš„è®¾å¤‡å¦‚ä½•ä½¿ç”¨ï¼Ÿ

**A**: ç›´æ¥ä¼ ç»™ `.to(device)`ï¼š

```python
from utils import get_device

device = get_device('auto')

# æ¨¡å‹
model = model.to(device)

# å¼ é‡
images = images.to(device)
labels = labels.to(device)

# ä¼˜åŒ–å™¨ï¼ˆè‡ªåŠ¨é€‚é…ï¼‰
optimizer = torch.optim.Adam(model.parameters())
```

### Q3: ä¸ºä»€ä¹ˆ `clear_memory()` åæ˜¾å­˜æ²¡æœ‰å®Œå…¨é‡Šæ”¾ï¼Ÿ

**A**: PyTorch çš„æ˜¾å­˜ç®¡ç†æœºåˆ¶ï¼š

1. **ç¼“å­˜æ± **ï¼šPyTorch ä¼šä¿ç•™éƒ¨åˆ†æ˜¾å­˜ç”¨äºåŠ é€Ÿåç»­åˆ†é…
2. **çœŸæ­£é‡Šæ”¾**ï¼šåªæœ‰åœ¨å¯¹è±¡è¢«åˆ é™¤åæ‰ä¼šé‡Šæ”¾

**æ­£ç¡®çš„æ¸…ç†æ–¹å¼**ï¼š

```python
from utils import clear_memory

# 1. åˆ é™¤å¯¹è±¡
del model
del optimizer

# 2. æ¸…ç†ç¼“å­˜
clear_memory()

# 3. (å¯é€‰) å¼ºåˆ¶åƒåœ¾å›æ”¶
import gc
gc.collect()
```

### Q4: `validate_tensor()` ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ï¼Ÿ

**A**: ä¸»è¦ç”¨äºè°ƒè¯•ä»¥ä¸‹é—®é¢˜ï¼š

- **æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±**ï¼šæ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸º NaN/Inf
- **æ•°å€¼ä¸ç¨³å®š**ï¼šæ£€æŸ¥ä¸­é—´å±‚è¾“å‡º
- **æŸå¤±å¼‚å¸¸**ï¼šæ£€æŸ¥æŸå¤±å€¼

**ä¸æ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨**ï¼ˆæ€§èƒ½å¼€é”€ï¼‰ã€‚

### Q5: `count_parameters()` çš„ä¸¤ç§æ¨¡å¼æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**:

```python
from utils import count_parameters

model = YourModel()

# æ¨¡å¼ 1: ç»Ÿè®¡æ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬å†»ç»“çš„ï¼‰
total = count_parameters(model, trainable_only=False)

# æ¨¡å¼ 2: åªç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
trainable = count_parameters(model, trainable_only=True)

# å…³ç³»
frozen = total - trainable
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- **è¿ç§»å­¦ä¹ **ï¼š`trainable_only=True` æŸ¥çœ‹éœ€è¦è®­ç»ƒçš„å‚æ•°
- **æ¨¡å‹å¯¹æ¯”**ï¼š`trainable_only=False` æŸ¥çœ‹æ¨¡å‹æ€»å¤§å°

### Q6: å¦‚ä½•åœ¨å¤š GPU è®­ç»ƒä¸­ä½¿ç”¨è¿™äº›å·¥å…·ï¼Ÿ

**A**:

**DataParallel æ¨¡å¼**ï¼š

```python
from utils import get_device, count_parameters

# ä¸»è®¾å¤‡
device = get_device('cuda:0')

# æ¨¡å‹åŒ…è£…
model = nn.DataParallel(model, device_ids=[0, 1, 2, 3])
model = model.to(device)

# ç»Ÿè®¡å‚æ•°ï¼ˆä½¿ç”¨ model.moduleï¼‰
total_params = count_parameters(model.module)
```

**DistributedDataParallel æ¨¡å¼**ï¼š

```python
from utils import get_device

# æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„ GPU
local_rank = int(os.environ['LOCAL_RANK'])
device = get_device(f'cuda:{local_rank}')

model = model.to(device)
model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])
```

### Q7: JSON ä¿å­˜/åŠ è½½æ”¯æŒå“ªäº›æ•°æ®ç±»å‹ï¼Ÿ

**A**: æ ‡å‡† JSON ç±»å‹ï¼š

**æ”¯æŒçš„ç±»å‹**ï¼š
- `str`, `int`, `float`, `bool`
- `list`, `dict`
- `None`

**ä¸æ”¯æŒçš„ç±»å‹**ï¼ˆéœ€è¦è½¬æ¢ï¼‰ï¼š
- `numpy.ndarray` â†’ è½¬ä¸º `list`
- `torch.Tensor` â†’ è½¬ä¸º `list`
- `datetime` â†’ è½¬ä¸º `str`

**ç¤ºä¾‹**ï¼š

```python
from utils import save_dict_to_json
import numpy as np

# é”™è¯¯ç¤ºä¾‹
data = {"array": np.array([1, 2, 3])}  # ä¼šæŠ¥é”™

# æ­£ç¡®ç¤ºä¾‹
data = {"array": np.array([1, 2, 3]).tolist()}  # è½¬ä¸º list
save_dict_to_json(data, "results.json")
```

---

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. æ ‡å‡†è®­ç»ƒè„šæœ¬æ¨¡æ¿

```python
from utils import (
    setup_logging,
    setup_config,
    set_random_seed,
    get_device,
    load_dataset_info,
    count_parameters,
    log_memory_usage,
    format_time
)
from loguru import logger
import time

def main():
    # 1. é…ç½®æ—¥å¿—
    setup_logging(log_dir='./logs', console_level='INFO', file_level='DEBUG')

    # 2. åŠ è½½é…ç½®
    logger.info("åŠ è½½é…ç½®...")
    config = setup_config(DEFAULT_CONFIG, 'config.yaml', {})

    # 3. è®¾ç½®éšæœºç§å­
    logger.info(f"è®¾ç½®éšæœºç§å­: {config.experiment.seed}")
    set_random_seed(
        seed=config.experiment.seed,
        enable_cudnn_benchmark=config.experiment.enable_cudnn_benchmark
    )

    # 4. è·å–è®¾å¤‡
    device = get_device('auto')

    # 5. åŠ è½½æ•°æ®
    logger.info("åŠ è½½æ•°æ®é›†...")
    dataset_info = load_dataset_info(
        dataset_name=config.dataset.name,
        data_path=config.dataset.data_path
    )

    # 6. åˆ›å»ºæ¨¡å‹
    logger.info("åˆ›å»ºæ¨¡å‹...")
    model = create_model(
        input_channels=dataset_info['channel'],
        num_classes=dataset_info['num_classes']
    ).to(device)

    # 7. ç»Ÿè®¡å‚æ•°
    total_params = count_parameters(model)
    logger.info(f"æ¨¡å‹å‚æ•°: {total_params:,}")

    # 8. è®°å½•å†…å­˜
    log_memory_usage("æ¨¡å‹åŠ è½½å")

    # 9. è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()

    trainer.fit(train_loader, val_loader, epochs=config.training.epochs)

    total_time = time.time() - start_time
    logger.success(f"è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {format_time(total_time)}")

    # 10. æ¸…ç†
    log_memory_usage("è®­ç»ƒç»“æŸ")

if __name__ == '__main__':
    main()
```

### 2. å†…å­˜ç›‘æ§æœ€ä½³å®è·µ

```python
from utils import log_memory_usage, clear_memory

# è®­ç»ƒå‰
log_memory_usage("è®­ç»ƒå‰")

# è®­ç»ƒå¾ªç¯
for epoch in range(100):
    # ... è®­ç»ƒä»£ç  ...

    # éªŒè¯
    model.eval()
    with torch.no_grad():
        # ... éªŒè¯ä»£ç  ...
        pass

    # éªŒè¯åæ¸…ç†
    clear_memory()

    # æ¯ 10 ä¸ª epoch è®°å½•ä¸€æ¬¡
    if (epoch + 1) % 10 == 0:
        log_memory_usage(f"Epoch {epoch + 1}")

# è®­ç»ƒå
log_memory_usage("è®­ç»ƒå")
```

### 3. ç»“æœä¿å­˜æœ€ä½³å®è·µ

```python
from utils import save_dict_to_json, format_time, get_time
import time

# è®­ç»ƒè¿‡ç¨‹
start_time = time.time()
best_val_acc = 0.0

for epoch in range(100):
    # ... è®­ç»ƒä»£ç  ...
    if val_acc > best_val_acc:
        best_val_acc = val_acc

total_time = time.time() - start_time

# ä¿å­˜ç»“æœ
results = {
    "experiment_name": config.experiment.name,
    "timestamp": get_time("%Y-%m-%d %H:%M:%S"),
    "dataset": config.dataset.name,
    "model": config.model.name,
    "metrics": {
        "best_val_acc": float(best_val_acc),
        "final_loss": float(final_loss)
    },
    "hyperparameters": {
        "epochs": config.training.epochs,
        "batch_size": config.training.batch_size,
        "learning_rate": config.training.lr,
        "optimizer": config.training.optimizer
    },
    "system": {
        "total_time": format_time(total_time),
        "device": str(device),
        "seed": config.experiment.seed
    }
}

# ä¿å­˜åˆ°æ–‡ä»¶
timestamp = get_time("%Y%m%d_%H%M%S")
save_dict_to_json(results, f"./results/result_{timestamp}.json")
```

---

## ğŸ¯ æ€»ç»“

`helpers` æ¨¡å—çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

1. **æ—¶é—´å·¥å…·**ï¼šä¼˜é›…çš„æ—¶é—´æ ¼å¼åŒ–ï¼Œæ–¹ä¾¿æ—¥å¿—å’Œæ–‡ä»¶å‘½å
2. **å¯å¤ç°æ€§**ï¼šä¸€é”®è®¾ç½®å…¨å±€éšæœºç§å­ï¼Œæ”¯æŒæ€§èƒ½/å¯å¤ç°æ€§æƒè¡¡
3. **è®¾å¤‡ç®¡ç†**ï¼šæ™ºèƒ½è®¾å¤‡é€‰æ‹©ï¼Œè‡ªåŠ¨å›é€€æœºåˆ¶
4. **å†…å­˜ç›‘æ§**ï¼šå®æ—¶ç›‘æ§ GPU æ˜¾å­˜ï¼ŒåŠæ—¶æ¸…ç†ç¼“å­˜
5. **è°ƒè¯•å·¥å…·**ï¼šå¼ é‡éªŒè¯ã€å‚æ•°ç»Ÿè®¡ï¼Œå¿«é€Ÿå®šä½é—®é¢˜
6. **æ ¼å¼åŒ– IO**ï¼šæ ‡å‡†åŒ–çš„ JSON ä¿å­˜/åŠ è½½ï¼Œæ–¹ä¾¿ç»“æœç®¡ç†

è¿™äº›å·¥å…·å‡½æ•°è®© PyTorch è®­ç»ƒè„šæœ¬æ›´åŠ ä¼˜é›…å’Œå¥å£®ï¼ğŸ‰
