#!/usr/bin/python
# -*- coding:utf-8 -*-
"""
Created on 2025/11/1 23:40
@author  : William_Trouvaille
@function: TODO
"""

import sys
import os
import time
import argparse
import traceback
from loguru import logger
from tqdm import tqdm

# 1. --- PyTorch æ ¸å¿ƒå¯¼å…¥ ---
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# 2. --- å¯¼å…¥æˆ‘ä»¬æ‰€æœ‰çš„ utils å·¥å…· ---
from utils import (
    # æ—¥å¿—
    setup_logging,

    # é…ç½®
    setup_config, print_config, save_config_to_yaml,

    # é€šçŸ¥
    NtfyNotifier,

    # æ£€æŸ¥ç‚¹
    CheckpointManager,

    # æ•°æ®
    load_dataset_info,

    # è¾…åŠ©
    set_random_seed, get_device, clear_memory, count_parameters,

    # è¿›åº¦æ¡
    Progress
)


# 3. --- é¡¹ç›®ç‰¹å®šå®šä¹‰ (æœ¬åº”åœ¨ model.py, config.py ä¸­) ---

class SimpleMNISTConvNet(nn.Module):
    """ä¸€ä¸ªä¸º MNIST (1x28x28) è®¾è®¡çš„ç®€å• CNN"""
    def __init__(self, num_classes=10):
        super().__init__()
        # (Batch, 1, 28, 28)
        self.conv1 = nn.Conv2d(1, 16, 5, 1, 2) # (Batch, 16, 28, 28)
        self.pool1 = nn.MaxPool2d(2)          # (Batch, 16, 14, 14)
        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2) # (Batch, 32, 14, 14)
        self.pool2 = nn.MaxPool2d(2)          # (Batch, 32, 7, 7)
        self.fc_input_features = 32 * 7 * 7
        self.fc1 = nn.Linear(self.fc_input_features, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool1(self.relu(self.conv1(x)))
        x = self.pool2(self.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_features)
        x = self.relu(self.fc1(x))
        return self.fc2(x)

def get_project_defaults() -> dict:
    """å®šä¹‰æœ¬é¡¹ç›®ï¼ˆMNIST å®éªŒï¼‰çš„é»˜è®¤å‚æ•°"""
    return {
        'experiment': {
            'name': 'mnist_tracker_comparison',
            'seed': 42,
        },
        'dataset': {
            'name': 'MNIST',
            'data_path': './data',
        },
        'dataloader': {
            # (æ€§èƒ½) æ¨èä½¿ç”¨ > 0
            'num_workers': 4 if sys.platform != "win32" else 0,
            'pin_memory': True,
        },
        'model': {
            'name': 'SimpleMNISTConvNet',
        },
        'training': {
            'epochs': 20,  # ä¿æŒè¾ƒçŸ­æ—¶é—´ä»¥ä¾¿äºæµ‹è¯•
            'lr': 0.01,
            'batch_size': 128,
            'optimizer': 'SGD'
        },
        'logging': {
            'log_dir': './logs',
            'console_level': 'INFO',
            'file_level': 'DEBUG'
        },
        'checkpoint': {
            'save_dir': './checkpoints',
            'max_to_keep': 3
        },
        'ntfy': {
            'enabled': True # è®¾ä¸º False å¯ç¦ç”¨ ntfy
        }
    }

def parse_arguments() -> dict:
    """å®šä¹‰å’Œè§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="MNIST è®­ç»ƒä¸è¿›åº¦æ¡å¯¹æ¯”å®éªŒ")

    parser.add_argument(
        '-c', '--config',
        type=str,
        default='config.yaml',
        help='é…ç½®æ–‡ä»¶çš„è·¯å¾„'
    )
    parser.add_argument(
        '--training.epochs',
        type=int,
        help='è¦†ç›–è®­ç»ƒè½®æ•°'
    )
    parser.add_argument(
        '--training.batch_size',
        type=int,
        help='è¦†ç›–æ‰¹æ¬¡å¤§å°'
    )

    args = parser.parse_args()
    return vars(args)


# 4. --- æ ¸å¿ƒè®­ç»ƒä¸è¯„ä¼°é€»è¾‘ ---

def train_epoch_with_tracker(epoch: int, model: nn.Module, loader: DataLoader,
                             optimizer: optim.Optimizer, criterion: nn.Module,
                             device: torch.device):
    """(å®éªŒ A) ä½¿ç”¨é«˜æ€§èƒ½ Progress è¿›è¡Œè®­ç»ƒ"""

    model.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

    # 1. (å…³é”®) åŒ…è£… DataLoader
    with Progress(
            loader,
            description=f"Epoch {epoch+1} (A: Tracker)",
            leave=False,
            device=device
    ) as tracker:

        for images, labels in tracker:
            # 2. å°†æ•°æ®ç§»è‡³è®¾å¤‡
            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

            # 3. æ ‡å‡†è®­ç»ƒæ­¥éª¤
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels) # (è¿™æ˜¯ä¸€ä¸ª Tensor)
            loss.backward()
            optimizer.step()

            # 4. (å…³é”®) æ›´æ–° Tracker
            # è¿™æ˜¯ä¸€ä¸ªå»‰ä»·ã€éé˜»å¡çš„è°ƒç”¨
            tracker.update({'loss': loss})

    # 5. è¿”å›æœ€ç»ˆçš„å¹³å‡ loss
    return tracker.get_final_metrics().get('loss', 0.0)


def train_epoch_without_tracker(epoch: int, model: nn.Module, loader: DataLoader,
                                optimizer: optim.Optimizer, criterion: nn.Module,
                                device: torch.device):
    """(å®éªŒ B) ä¸ä½¿ç”¨ä»»ä½•è¿›åº¦æ¡ï¼Œä»…åœ¨å†…éƒ¨å¾ªç¯"""

    model.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    total_loss = 0.0

    # 1. (å…³é”®) ç›´æ¥è¿­ä»£
    for images, labels in loader:
        # 2. å°†æ•°æ®ç§»è‡³è®¾å¤‡
        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

        # 3. æ ‡å‡†è®­ç»ƒæ­¥éª¤
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 4. (å…³é”®) å¿…é¡»è°ƒç”¨ .item() æ¥åŒæ­¥å¹¶ç´¯åŠ 
        total_loss += loss.item()

    # 5. è¿”å›æœ€ç»ˆçš„å¹³å‡ loss
    return total_loss / len(loader)


def evaluate(model: nn.Module, loader: DataLoader,
             criterion: nn.Module, device: torch.device) -> (float, float):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    total_loss = 0.0
    total_correct = 0
    total_samples = 0

    # è¯„ä¼°æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
    with torch.no_grad():
        # è¯„ä¼°å¾ªç¯é€šå¸¸å¾ˆå¿«ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ TQDM
        for images, labels in tqdm(loader, desc="Validating", leave=False, ncols=100):
            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

            outputs = model(images)
            loss = criterion(outputs, labels)

            _, predicted = torch.max(outputs.data, 1)
            total_correct += (predicted == labels).sum().item()
            total_loss += loss.item() * images.size(0)
            total_samples += labels.size(0)

    avg_loss = total_loss / total_samples
    avg_acc = (total_correct / total_samples) * 100
    return avg_loss, avg_acc

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    # --- 1. åˆå§‹åŒ–è®¾ç½® (æ—¥å¿—, é…ç½®, é€šçŸ¥) ---

    # (å…³é”®) å¿…é¡»å…ˆè®¾ç½®ä¸€ä¸ªä¸´æ—¶æ—¥å¿—
    setup_logging(log_dir="../logs", console_level="INFO")

    cmd_args = parse_arguments()
    default_config = get_project_defaults()

    # (å…³é”®) `setup_config` ä¼šåŠ è½½ YAML å’Œ CMD å‚æ•°ï¼Œè¦†ç›–é»˜è®¤å€¼
    config = setup_config(
        default_config=default_config,
        yaml_config_path=cmd_args['config'],
        cmd_args=cmd_args
    )

    # (å…³é”®) `setup_config` å®Œæˆåï¼Œæˆ‘ä»¬æœ‰äº†æœ€ç»ˆçš„æ—¥å¿—è·¯å¾„
    # å†æ¬¡è°ƒç”¨ setup_logging ä»¥ä½¿ç”¨*æ­£ç¡®*çš„é…ç½®
    setup_logging(
        log_dir=config.logging.log_dir,
        console_level=config.logging.console_level,
        file_level=config.logging.file_level
    )

    # (å¯é€‰) å­˜æ¡£æœ¬æ¬¡è¿è¡Œçš„æœ€ç»ˆé…ç½®
    # run_config_path = os.path.join(config.logging.log_dir, "run_config.yaml")
    # save_config_to_yaml(config, run_config_path)

    # åˆå§‹åŒ– Ntfy
    notifier = NtfyNotifier()
    ntfy_enabled = config.ntfy.enabled

    # --- 2. æ ¸å¿ƒè®­ç»ƒé€»è¾‘ (ä½¿ç”¨å®Œæ•´çš„é”™è¯¯å¤„ç†) ---

    # åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    ckpt_manager = CheckpointManager(
        save_dir=config.checkpoint.save_dir,
        max_to_keep=config.checkpoint.max_to_keep
    )

    # å‡†å¤‡ä¿å­˜ä¸­æ–­æ—¶çš„çŠ¶æ€
    interrupt_state = {}

    try:
        # --- 3. å®éªŒè®¾ç½® (è®¾å¤‡, ç§å­, æ•°æ®) ---

        if ntfy_enabled:
            notifier.notify_start(f"å®éªŒ {config.experiment.name} å·²å¼€å§‹ã€‚")

        set_random_seed(config.experiment.seed)
        device = get_device() # 'auto'
        print_config(config, "MNIST å®éªŒé…ç½®")

        # åŠ è½½æ•°æ®
        data_info = load_dataset_info(
            dataset_name=config.dataset.name,
            data_path=config.dataset.data_path
        )

        # (å…³é”®) main.py è´Ÿè´£åˆ›å»º DataLoaderï¼Œä½¿ç”¨ config ä¸­çš„æ€§èƒ½å‚æ•°
        train_loader = DataLoader(
            data_info['dst_train'],
            batch_size=config.training.batch_size,
            shuffle=True,
            num_workers=config.dataloader.num_workers,
            pin_memory=config.dataloader.pin_memory,
            persistent_workers=True if config.dataloader.num_workers > 0 else False
        )
        test_loader = DataLoader(
            data_info['dst_test'],
            batch_size=config.training.batch_size * 2, # è¯„ä¼°æ—¶ batch
            shuffle=False,
            num_workers=config.dataloader.num_workers,
            pin_memory=config.dataloader.pin_memory
        )

        # --- 4. å®éªŒ A: ä½¿ç”¨ Progress ---
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹å®éªŒ A: ä½¿ç”¨ Progress")
        logger.info("=" * 60)

        # åˆå§‹åŒ–æ¨¡å‹
        model_a = SimpleMNISTConvNet(num_classes=data_info['num_classes']).to(device)
        optimizer_a = optim.SGD(model_a.parameters(), lr=config.training.lr)
        criterion = nn.CrossEntropyLoss().to(device)

        logger.info(f"æ¨¡å‹å‚æ•°: {count_parameters(model_a):,}")

        # å°è¯•æ¢å¤
        start_epoch = 0
        best_acc = 0.0

        logger.info("æ­£åœ¨æ£€æŸ¥å®éªŒ A çš„æ£€æŸ¥ç‚¹...")
        latest_ckpt = ckpt_manager.load_latest_checkpoint()
        if latest_ckpt:
            try:
                model_a.load_state_dict(latest_ckpt['model_state'])
                optimizer_a.load_state_dict(latest_ckpt['optimizer_state'])
                start_epoch = latest_ckpt['epoch'] + 1
                best_acc = latest_ckpt.get('best_acc', 0.0)
                logger.success(f"æˆåŠŸä» Epoch {start_epoch - 1} æ¢å¤è®­ç»ƒã€‚")
            except Exception as e:
                logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}ã€‚å°†ä»å¤´å¼€å§‹ã€‚")

        # è®°å½•æ—¶é—´
        exp_a_start_time = time.monotonic()

        for epoch in range(start_epoch, config.training.epochs):

            # (è°ƒç”¨ A)
            train_loss = train_epoch_with_tracker(
                epoch, model_a, train_loader, optimizer_a, criterion, device
            )

            val_loss, val_acc = evaluate(model_a, test_loader, criterion, device)

            logger.info(
                f"Epoch {epoch+1}/{config.training.epochs} | "
                f"Train Loss: {train_loss:.4f} | "
                f"Val Loss: {val_loss:.4f} | "
                f"Val Acc: {val_acc:.2f}%"
            )

            # (å…³é”®) å‡†å¤‡ state å­—å…¸
            state = {
                'epoch': epoch,
                'model_state': model_a.state_dict(),
                'optimizer_state': optimizer_a.state_dict(),
                'best_acc': best_acc,
                'config': config.to_dict() # ä¿å­˜é…ç½®å¿«ç…§
            }
            # (å…³é”®) æ›´æ–°ä¸­æ–­çŠ¶æ€
            interrupt_state = state

            # ä¿å­˜æ»šåŠ¨æ£€æŸ¥ç‚¹
            ckpt_manager.save_epoch_checkpoint(state, epoch)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_acc:
                logger.success(f"æ–°é«˜åˆ†! å‡†ç¡®ç‡ä» {best_acc:.2f}% æå‡åˆ° {val_acc:.2f}%")
                best_acc = val_acc
                state['best_acc'] = best_acc # æ›´æ–° state
                ckpt_manager.save_best_model(state, best_acc)

        exp_a_end_time = time.monotonic()
        time_a = exp_a_end_time - exp_a_start_time


        # --- 5. å®éªŒ B: ä¸ä½¿ç”¨ Progress ---
        logger.info("=" * 60)
        logger.info("ğŸ¢ å¼€å§‹å®éªŒ B: ä¸ä½¿ç”¨ä»»ä½•è¿›åº¦æ¡")
        logger.info("=" * 60)

        # (å…³é”®) é‡ç½®æ‰€æœ‰çŠ¶æ€
        logger.warning("æ­£åœ¨é‡ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨ä»¥è¿›è¡Œå…¬å¹³æ¯”è¾ƒ...")
        clear_memory()

        model_b = SimpleMNISTConvNet(num_classes=data_info['num_classes']).to(device)
        optimizer_b = optim.SGD(model_b.parameters(), lr=config.training.lr)

        exp_b_start_time = time.monotonic()

        for epoch in range(config.training.epochs):
            # (è°ƒç”¨ B)
            train_loss = train_epoch_without_tracker(
                epoch, model_b, train_loader, optimizer_b, criterion, device
            )
            # (ä¸æ‰“å°æ—¥å¿—ï¼Œæ¨¡æ‹Ÿçº¯ç²¹çš„ã€æ—  I/O çš„è®­ç»ƒ)
            # logger.info(f"Epoch {epoch+1} (B) | Train Loss: {train_loss:.4f}")

        exp_b_end_time = time.monotonic()
        time_b = exp_b_end_time - exp_b_start_time


        # --- 6. å®éªŒç»“è®º ---
        logger.info("=" * 60)
        logger.info("ğŸ“Š å®éªŒå¯¹æ¯”ç»“è®º")
        logger.info("=" * 60)

        logger.info(f"å®éªŒ A (ä½¿ç”¨ Progress) æ€»è€—æ—¶: {time_a:.3f} ç§’")
        logger.info(f"å®éªŒ B (æ—  I/O)             æ€»è€—æ—¶: {time_b:.3f} ç§’")

        overhead = time_a - time_b
        overhead_percent = (overhead / time_b) * 100

        if overhead > 0:
            logger.warning(f"Progress å¸¦æ¥äº† {overhead:.3f} ç§’çš„å¼€é”€ ({overhead_percent:+.2f}%)")
        else:
            logger.success(f"Progress å¼€é”€å¯å¿½ç•¥ä¸è®¡ ({overhead:.3f}s)")

        if ntfy_enabled:
            notifier.notify_success(
                f"å®éªŒ {config.experiment.name} å·²å®Œæˆã€‚\n\n"
                f"**Tracker (A):** {time_a:.3f}s\n"
                f"**No I/O (B):** {time_b:.3f}s\n"
                f"**Overhead:** {overhead_percent:+.2f}%"
            )

    except KeyboardInterrupt:
        # --- 7. (å…³é”®) å¤„ç† Ctrl+C ---
        logger.critical("æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ (KeyboardInterrupt)ï¼")
        if interrupt_state:
            logger.info("æ­£åœ¨ä¿å­˜æœ€åçš„ä¸­æ–­æ£€æŸ¥ç‚¹...")
            ckpt_manager.save_interrupt_checkpoint(interrupt_state)

        if ntfy_enabled:
            notifier.notify_error(
                message=f"å®éªŒ {config.experiment.name} è¢«ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­ã€‚",
                error_details="KeyboardInterrupt"
            )

    except Exception as e:
        # --- 8. (å…³é”®) å¤„ç†æ‰€æœ‰å…¶ä»–å¼‚å¸¸ ---
        logger.error(f"å®éªŒ {config.experiment.name} å› æœªæ•è·çš„å¼‚å¸¸è€Œå¤±è´¥ï¼")

        # è·å–å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
        error_details = traceback.format_exc()
        logger.exception(error_details) # loguru.exception ä¼šè‡ªåŠ¨è®°å½•å †æ ˆ

        if ntfy_enabled:
            notifier.notify_error(
                message=f"å®éªŒ {config.experiment.name} å¤±è´¥: {e}",
                error_details=error_details
            )

    finally:
        # --- 9. æœ€ç»ˆæ¸…ç† ---
        clear_memory()
        logger.info("=" * 60)
        logger.info(f"å®éªŒ {config.experiment.name} æ‰§è¡Œå®Œæ¯•ã€‚")
        logger.info("=" * 60)


if __name__ == '__main__':
    main()
